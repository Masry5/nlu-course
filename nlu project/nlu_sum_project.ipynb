{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDV3lCZHxwOe"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:10:48.773682Z",
     "iopub.status.busy": "2025-05-11T14:10:48.773177Z",
     "iopub.status.idle": "2025-05-11T14:12:13.456054Z",
     "shell.execute_reply": "2025-05-11T14:12:13.455366Z",
     "shell.execute_reply.started": "2025-05-11T14:10:48.773663Z"
    },
    "id": "vhrM7Kgyxydu",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0\n",
    "#!pip install -qU openai==1.61.0 wandb\n",
    "!pip install -qU json-repair==0.29.1\n",
    "!pip install -qU faker==35.2.0\n",
    "#!pip install -qU vllm==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:12:13.457387Z",
     "iopub.status.busy": "2025-05-11T14:12:13.457166Z",
     "iopub.status.idle": "2025-05-11T14:12:40.413370Z",
     "shell.execute_reply": "2025-05-11T14:12:40.412689Z",
     "shell.execute_reply.started": "2025-05-11T14:12:13.457369Z"
    },
    "id": "8526F9lVKilu",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 357, done.\u001b[K\n",
      "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
      "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
      "remote: Total 357 (delta 77), reused 292 (delta 66), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (357/357), 9.64 MiB | 35.40 MiB/s, done.\n",
      "Resolving deltas: 100% (77/77), done.\n",
      "Obtaining file:///kaggle/working/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.48.3)\n",
      "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
      "Collecting gradio<=5.25.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
      "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading sse_starlette-2.3.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.7.5)\n",
      "Collecting fire (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Collecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.3)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.10.2.post1)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.16)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7.1)\n",
      "Collecting ffmpy (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.15)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Collecting python-multipart>=0.0.18 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (14.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (14.0.0)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.19.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Downloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.3.4-py3-none-any.whl (10 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Building wheels for collected packages: llamafactory, fire\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26939 sha256=603e62951ddfe78d2e03bbab89f9d6acc106ba61726eefc09a0d81e46b4e40bb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8jgtm315/wheels/96/d8/b2/8fc665ed70525080a50f3ff8538833c6f74cd48eb82195d0f8\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=591f77ca41951071f4651daba277098635dc903a60b807ab50c1fbef5927fae1\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built llamafactory fire\n",
      "Installing collected packages: uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pydantic-core, groovy, fire, ffmpy, av, anyio, starlette, pydantic, tyro, sse-starlette, safehttpx, gradio-client, fastapi, trl, gradio, llamafactory\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.1\n",
      "    Uninstalling pydantic_core-2.33.1:\n",
      "      Successfully uninstalled pydantic_core-2.33.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.3\n",
      "    Uninstalling pydantic-2.11.3:\n",
      "      Successfully uninstalled pydantic-2.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-4.9.0 av-14.3.0 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 llamafactory-0.9.3.dev0 pydantic-2.10.6 pydantic-core-2.27.2 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.2 sse-starlette-2.3.4 starlette-0.46.2 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:12:40.414392Z",
     "iopub.status.busy": "2025-05-11T14:12:40.414185Z",
     "iopub.status.idle": "2025-05-11T14:12:43.629196Z",
     "shell.execute_reply": "2025-05-11T14:12:43.628664Z",
     "shell.execute_reply.started": "2025-05-11T14:12:40.414373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets\n",
    "#path = f\"/kaggle/working/multi-lexsum/multi_lexsum\"\n",
    "path = f\"/kaggle/input/multi-lexsum/multi_lexsum\"\n",
    "ds   = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:12:43.630638Z",
     "iopub.status.busy": "2025-05-11T14:12:43.630339Z",
     "iopub.status.idle": "2025-05-11T14:12:52.539038Z",
     "shell.execute_reply": "2025-05-11T14:12:52.538306Z",
     "shell.execute_reply.started": "2025-05-11T14:12:43.630620Z"
    },
    "id": "65ZhrnavyL0Q",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmedoahmed0120\u001b[0m (\u001b[33mmedoahmed0120-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `legal` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `legal`\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import userdata\n",
    "import wandb\n",
    "\n",
    "wandb.login(key='bb588365ecjghjlkflgfdgfdgfdgdf')\n",
    "hf_token = 'hf_NKNJDwtXMdPgfdgdshrrrfddfhdf'\n",
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLdOa2oSyuBE"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:12:52.540511Z",
     "iopub.status.busy": "2025-05-11T14:12:52.539835Z",
     "iopub.status.idle": "2025-05-11T14:13:03.061210Z",
     "shell.execute_reply": "2025-05-11T14:13:03.060602Z",
     "shell.execute_reply.started": "2025-05-11T14:12:52.540487Z"
    },
    "id": "y8utsMq7y_TP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from datetime import datetime\n",
    "\n",
    "import json_repair\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "data_dir = \"/kaggle/working/\"\n",
    "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "device = \"cuda\"\n",
    "torch_dtype = None\n",
    "\n",
    "def parse_json(text):\n",
    "    try:\n",
    "        return json_repair.loads(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBbGFkVEH1X-"
   },
   "source": [
    "## Format Finetuning Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:13:03.062317Z",
     "iopub.status.busy": "2025-05-11T14:13:03.061892Z",
     "iopub.status.idle": "2025-05-11T14:13:26.305781Z",
     "shell.execute_reply": "2025-05-11T14:13:26.305178Z",
     "shell.execute_reply.started": "2025-05-11T14:13:03.062299Z"
    },
    "id": "bn4uUuDOIO3D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sft_data_path = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
    "\n",
    "system_message = \"\\n\".join([\n",
    "    \"You are a professional NLP data parser.\",\n",
    "    \"Follow the provided `Task` by the user to generate the `Output JSON`.\",\n",
    "    \"Do not generate any introduction or conclusion.\"\n",
    "])\n",
    "\n",
    "def prepare_data(ds):\n",
    "    llm_finetuning_data = []\n",
    "    \n",
    "    for rec in ds:\n",
    "        combined_source = ''\n",
    "        if len(rec['sources']) > 3:\n",
    "            combined_source = '\\n'.join(rec['sources'][:3])\n",
    "        else:\n",
    "            combined_source = '\\n'.join(rec['sources'])\n",
    "        \n",
    "        llm_finetuning_data.append({\n",
    "            \"system\": system_message,\n",
    "            \"instruction\": \"\\n\".join([\n",
    "                \"# Document:\",\n",
    "                combined_source,\n",
    "                \"\",\n",
    "                \"# Task: summarize\",\n",
    "                \"# Output JSON:\",\n",
    "                \"```json\"\n",
    "            ]),\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"\\n\".join([\n",
    "                \"```json\",\n",
    "                # Fix the key typo\n",
    "                json.dumps({\n",
    "                    'Summary_long': rec[\"summary/long\"],\n",
    "                    'Summary_short': rec[\"summary/short\"]  # Fixed key\n",
    "                }),\n",
    "                \"```\"\n",
    "            ]),\n",
    "            \"history\": []\n",
    "        })\n",
    "    return llm_finetuning_data\n",
    "\n",
    "train_data=prepare_data(ds['train'])\n",
    "val_data=prepare_data(ds['validation'])\n",
    "test_data=prepare_data(ds['test'])\n",
    "\n",
    "#random.Random(101).shuffle(llm_finetunning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T14:13:26.314874Z",
     "iopub.status.busy": "2025-05-11T14:13:26.314677Z",
     "iopub.status.idle": "2025-05-11T14:13:26.326121Z",
     "shell.execute_reply": "2025-05-11T14:13:26.325644Z",
     "shell.execute_reply.started": "2025-05-11T14:13:26.314856Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are a professional NLP data parser.\\nFollow the provided `Task` by the user to generate the `Output JSON`.\\nDo not generate any introduction or conclusion.',\n",
       " 'instruction': '# Document:\\nCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 1 of 6\\n\\nIN\\n\\nTHE\\n\\nUNITED\\n\\nSTATES\\n\\nDISTRICT\\n\\nFILD COUR T\\n\\nP19\\n\\n.05\\n\\nNl\\n\\nel\\n\\n.s\\n\\nFOR THE SOUTHERN DISTRICT OF ALABAMA\\n\\nSOUTHERN DIVISION\\n\\nEQUAL EMPLOYMENT OPPORTUNITY ]\\n\\nCOMMISSION, ]\\n\\n] Plaintiff, ] Civil Action No. OSS- 0\\'53a -~\\n\\nv.\\n\\n]\\n\\n]\\nCOMPLAINT\\n\\n] HOUSE OF PHILADELPHIA CENTER, INC . ]\\n\\nJURY TRIAL DEMAND\\n\\nDefendant .\\n\\n]\\n]\\n] ]\\n\\nNATURE OF THE ACTION This is an action under Title VII of the Civil Rights Act of 1964 and Title I of the Civil Rights Act of 1991 to correct unlawful employment practices on the basis of sex and to provide appropriate relief to Sharonda Griffin who was adversely affected by such practices . The Commission alleges that the Defendant discriminated against Sharonda Griffin because of her sex, female .\\n\\n1\\n\\n\\x0cCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 2 of 6\\nJURISDICTION AND VENU E 1 . Jurisdiction of this Court is invoked pursuant to 28 U .S .C. §§ 451, 1331, 1337, 1343 and 1345 . This action is authorized and instituted pursuant to §§ 703, 706(f)(1) and (3) of Title VII of the Civil Rights Act of 1964, as amended, 42 U.S .C. § 2000e-2(a), 42 U .S .C . § 2000e-5(f)(l) and (3), and Section 102 of the Civil Rights Act of 1991, 42 U .S.C. Section 1981A . 2 . The unlawful employment practices alleged to be unlawful were committed within the jurisdiction of the United States District Court for the Southern District of Alabama, Southern Division .\\nPARTIES 3 . Plaintiff, the Equal Employment Opportunity Commission (the \"Commission\"), is the agency of the United States of America charged with the administration, interpretation and enforcement of Title VII, and is expressly authorized to bring this action by § 706(f)(1) and (3) of Title VII, 42 U.S.C. § 2000e-5(f)(l) and (3) . 4. At all relevant times, the Defendant, House of Philadelphia Center, Incorporated (\"House of Philadelphia\" or the \"Employer\") has continuously been doing business in the State of Alabama and the city of Irvington, and has continuously had at least 15 employees.\\n2\\n\\n\\x0cCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 3 of 6\\n5 . At all relevant times, the Defendant Employer has continuously been an employer engaged in an industry affecting commerce within the meaning of Sections 701(b), (g) and (h) of Title VII, 42 U .S .C . § 2000e-(b), (g) and (h) .\\nSTATEMENT OF CLAIM S 6 . More than thirty days prior to the institution of this lawsuit, Sharonda Griffin filed a Charge of Discrimination with the Commission alleging violations of Title VII by Defendant Employer. All conditions precedent to the institution of this lawsuit have been fulfilled. 7 . Since at least October 15, 2004, Defendant has engaged in unlawful employment practices at its Irvington, Alabama, facility in violation of Section 703 (a) of Title VII, 42 U .S.C. § 2000e-2(a). In particular, Defendant discharged Sharonda Griffin because she was pregnant . 8 . The effect of the practices complained of in paragraph 7 above has been to deprive Sharonda Griffin of equal employment opportunities and otherwise adversely affected her status as an employee, because of her sex, female . 9. The unlawful employment practices complained of in paragraphs 7 and 8 above were intentional .\\n3\\n\\n\\x0cCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 4 of 6\\n10. The unlawful employment practices complained of in paragraphs 7, 8 and 9 above were done with malice or with reckless indifference to the federally protected rights of Sharonda Griffin.\\nPRAYER FOR RELIE F WHEREFORE , the Commission respectfully requests that this Court :\\nA. Grant a permanent injunction enjoining the Defendant Employer, its officers, agents, successors, assigns and all persons in active concert or participation with it, from engaging in any employment practices which discriminate on the basis of sex.\\nB . Order the Defendant to institute and carry out policies, practices and programs which provide equal employment opportunities for all of its employees regardless of sex and which eradicate the effects of its past and present unlawful employment practices .\\nC . Order the Defendant to make whole Sharonda Griffin by providing appropriate relief, in amounts to be determined at trial, and other affirmative relief necessary to eradicate the effects of its unlawful employment practices .\\nD. Order Defendant Employer to make whole Sharonda Griffin by providing her with compensation for non-pecuniary losses resulting from the unlawful employment practices described in paragraphs 7, 8 and 9 above, including pain an d\\n4\\n\\n\\x0cCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 5 of 6\\nsuffering, emotional distress, humiliation, isolation, depression, and loss of enjoyment of life, in amounts to be determined at trial .\\nE. Order Defendant Employer to pay Sharonda Griffin punitive damages for its malicious and reckless conduct described in paragraphs 7, 8 and 9 above, in amounts to be determined at trial .\\nF. Grant such further relief as the Court deems necessary and proper in the public interest .\\nG. Award the Commission its costs of this action . JURY TRIAL DEMAND\\nThe Commission requests a jury trial on all questions of fact raised by its complaint .\\nRespectfully submitted , JAMES L. LEE Deputy General Counsel GWENDOLYN YOUNG REAMS Associate General Counse l EQUAL EMPLOYMENT OPPORTUNITY\\nCOMMISSION 1801 \"L\" Street, N .W. Washington, DC 2050 7\\n5\\n\\n\\x0cCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 6 of 6\\na\\'rles E. G rrier CHARLES GUERRIER Regional Attorney Ohio State Bar ID #002354 6\\ns/ Mason D. Barrett MASON D . BARRETT Senior Trial Attorney Colorado State Bar ID #2130 9\\nEQUAL EMPLOYMENT OPPORTUNITY COMMISSION\\n113022 d Street South , Suite 2000 Birmingham, Alabama 35205-2886 Telephone : (205) 212-2047 Facsimile : (205) 212-204 1\\n6\\n\\n\\x0c\\nCase 1:05-cv-00530-KD-M Document 9-t Filed 02/21/2006 Page 1 of 6\\n\\nIN THE UNITED STATES DISTRICT COURT FOR THE SOUTHERN DISTRICT OF ALABAMA\\nSOUTHERN DIVISION\\n\\nEQUAL EMPLOYMENT\\n\\n)\\n\\nOPPORTUNITY COMMISSION,\\n\\n)\\n\\n)\\n\\n- plaintiff,.\\n\\n.).. :-.. ,\\n\\n..\\n\\nvs.\\nHOUSE OF PHILADELPHIA CENTER, INC.,\\nDefendant.\\n\\n)\\n\\nCIVIL ACTION NO. 05-0530-D\\n\\n)\\n\\n)\\n\\n)\\n\\n)\\n\\n)\\n\\n)\\n\\nCOMPLAINT IN INTERVENTION\\n\\n1. This claim arises under Title VII of the Civil Rights Act of 1964, as amended, and\\n\\nPlaintiff Sharonda Griffin also asserts state law claims. This Court has jurisdiction over this action\\n\\npursuant to Title 28 U.S.C. §§1331 and 1343. This Court has supplemental jurisdiction over\\n\\nplaintiffs state law claims pursuant to 28 U.S.C. § 1367.\\n\\n2. Plaintiff Sharonda Griffin (\"Plaintiff\") is a female over the age of nineteen (19)\\n\\n3. Defendant House of Philadelphia Center, Inc. (\"Defendant\") is a corporation doing business in the Southern District of Alabama.\\nAt all times relevant to the complaint, Defendant had fifteen or more employees. At all times relevant to the complaint, Plaintiff was employed by Defendant. During the course of Plaintiff’s employment, Defendant discriminated against Plaintiff on the basis of her sex and pregnancy. 7. Plaintiff’s supervisor made defamatory remarks about Plaintiff to her co-workers.\\n\\n\\x0cCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/21/2006 Page 2 of 6\\nPlaintiff was terminated from her employment on October ! 5, 2004. 9. Plaintiffs termination was based on her pregnancy. 10. In a letter 1o Plaintiff, dated October ! 5, 2004, Ms. Mamie H. Mackey, Director of the House of Philadelphia Center, Inc., stated, \"Please be advised thai your service is no longer needed at House o~Philadeiphia. Due to persofial, health reason which I have discussed with you. In the near future after you have the baby, I will consider you working for the company again. Your last working day will be Friday, October 15, 2004\". (Letter attached as Exhibit 1) 11. Plaintiff filed a timely Charge of Discrimination with the Equal Employment Opportunity Commission, (EEOC) Charge Number 130-2005-00806, in or about November of 2004. (EEOC Charge attached as Exhibit 2) 12. Plaintiff received a\"Cause Determination\" from the EEOC. (Determination attached as Exhibit 3) 13. The EEOC issued a determination that :\\nInvestigation revealed that Charging Party was discharged and informed she would be considered for re-employment after having her baby. Direct evidence indicates Charging Party’s pregnancy was a motivating factor in Respondent’s decision to discharge her. Evidence does not support the stated defense that the Charging Party voluntarily resigned. I fred reasonable cause to believe that the Charging Party was subjected to discriminatory discharge due to her pregnancy, in violalion of Title VII. 14. Thereafter, on or about September 19, 2005, the EEOC filed a lawsuit against the House of Philadelphia Center, Inc. 15. Plaintifftimely filed this complaint in intervention.\\nFirst Claim For Relief 16. Plaintiffincorporates by reference each of the foregoing allegations of fact as though\\n-2-\\n\\n\\x0cCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/2112006 Page 3 of 6\\nfully set forth herein. 17. Defendant discriminated against Plaintiff in the terms and condilions of her\\nemployment, at ieasl partly because of her sex and pregnancy. 18. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and\\n\" ¯ \": ~:onlinues to suffer emotional painand suffering, inconvenience, mental.anguish, loss of enjoyment of life, loss of income, and loss of employment benefits. 19. Defendant acled with malice or with reckless indifference to Plaintiff’s federally protected fights. WHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to this matter from her personnel files, back pay, front pay, compensatory and punitive damages, costs and attorneys’ fees and all other relief deemed appropriate by this Court and/or the\\nSecond Claim For Relief 20. Plaintiff incorporates by reference each of the foregoing allegations of fact as though fully set forth herein. 21. Defendant’s conduct created and/or condoned a hostile work environment for Plaintiff. 22. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and continues to suffer emotional pain and suffering, inconvenienee, mental anguish, and loss of enjoyment of life. 23. Defendant acted with malice or with reckless indifference to Plaintiffs federally protected rights.\\n-3-\\n\\n\\x0cCase 1:05-cv-00530-KD-M Document 9-1 Filed 02t21/2006 Page 4 of 6\\nWHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to Ibis mailer from her personnel files, back pay, front pay, compensatory and punitive damages, costs and atton~eys’ fees and all olher relief deemed appropriate by this Court and/or Ihe jury.\\n: -.. Third Claim For Relief - 24. Plaintiff incorporates by reference each of the foregoing allegations of fact as though fully set forth herein. 25. Defendant terminated Plaintiffs employment because of her sex and pregnancy. 26. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and continues to suffer emotional pain and suffering, inconvenience, mental anguish, and loss of enjoyment of life. 27. Defendant acted with malice or with reckless indifference to Plaintiffs federally protected rights. WHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to this matter from her personnel files, back pay, front pay, compensatory and punitive damages, costs and attorneys’ fees and all other relief deemed appropriate by this Court and/or the\\nSixth Claim For Relief 28. Plaintiffincoq~orates by reference each of the foregoing allegations of fact as though fully set forth herein. 29. Defendant made defamatory remarks about Plaintiff. 30. As a proximate result of defendant’s intentional and/or reckless conduct, Plaintiff\\n-4-\\n\\n\\x0cCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/21/2006 t-’age b ol u\\nsuffered and continues to suffer emotional pain and suffering, inconvenience, mental anguish, loss of enjoyment of life, loss of income, and loss of employment benefits.\\nWHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to this matter from her personnel files, back pay, front pay, compensatory and punitive. damages, coslg and aft0rne~s’ fees and all other relief deemed appropriate by this Court and/or the.\\njury.\\ns/Daniel A. Hannan DANIEL A. HANNAN (HANND4492) Attorney for Plaintiff OF COUNSEL: FRANKLIN & STEIN, P.C. 63 South Royal Street Suite 1109 Mobile, Alabama 36602 Phone: 251-433-0051 Fax: 251-433-3919 Email: dahlawyer@,hotmail, corn PLAINTIFF DEMANDS TRIAL BY JURY. s/Daniel A. Hannah\\n-5-\\n\\n\\x0cCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/21/2006 Page 6 of 6\\nComplaint in Intervention EEOC vs. House of Philadelphia Center, Inc. Civil Action No. 05-0530-D\\nCERTIFICATE OF SERVICE I hereby certify that on 9th day of F~bmary, 2006, 1 electronically filed the foregoing document with the Clerk of the Court using the CM/ECF system which will send notification of such filing to the following: Charles Guerrier, Esq., Mason D. Barrett, Esq. and Raymond L. Bell, Jr., Esq.\\ns/Daniel A. Hannan Federal Bar Number: HANND4492\\n-6-\\n\\n\\x0c\\ni\\n\\nIN THE UNITED STATES DISTRICT COURT FOR THE SOUTHERN DISTRICT OF ALABAlVlA\\nSOUTHERN D1VISION\\n\\nEQUAL EMPLOYMENT\\n\\n)\\n\\nOPPORTUNITY COMl\\\\1ISSION,\\n\\n)\\n\\n)\\n\\nPlaintiff,\\n\\n)\\n\\n)\\n\\nand\\n\\n)\\n\\n)\\n\\nSHARONDA GRIFFIN,\\n\\n)\\n\\n)\\n\\nP l a i n t i f f - I n t e n r e n or,\\n\\n)\\n\\nv.\\n\\n)\\n\\n)\\n\\nHOUSE OF PHILADELPIIIA CENTER,)\\n\\nINC.,\\n\\n)\\n\\n)\\n\\nDefendant.\\n\\n)\\n\\n)\\n\\nCIVIL ACTION NO: 1:05-530..KO-M\\n\\nCONSENT DECREE\\n\\nThe Equal Emplo)\\'lnent Opportunity Commission (HEEOC\" or HCommission\") filed this\\n\\naction against House of Philadelphia Center, Inc. (\"House of Philadelphia\" or \"Defendant\") on\\n\\nSeptember 19, 2005, in this Court, to enforce Title VII of the Civil Rights Act of 1964,42 U.S.C.\\n\\\\\\n§2000e et seq. (Title VII) and the Civil Rights Act of 1991, 42 U.S.C. §1981a. In the Complaint,\\n\\nthe Cornmission alleged that House of Philadelphia discriminated against Sharonda Griffin\\n\\n(\"Griffin\") on the basis of scx, femalc, by discharging Ms. Griffin due to her pregnancy, in\\n\\nviolation of Title VII.\\n\\nHouse of Philadelphia denies all allegations of unlawful or wrongful conduct raised in the\\n\\ncomplaint, and nothing stated in this Decree constitutes an admission of liability or wrongdoing\\n\\non the part of House of Philadelphia.\\n\\n\\x0cThe Parties do not object to the jurisdiction of the Court over this action and waive their rights to a hearing and the entry of findings of fact and conclusions of law. Venue is appropriate in the Southern District of Alabama (Southern Division). The parties agree that this Consent Decree is fair, reasonable, and does not violate the law or public policy. The rights of Ms. Griffin, House of Philadelphia, and the Commission are protected adequately by this Decree.\\nIn the interest of resolving this matter~ avoiding the expense of further litigation, and as a result of having engaged in comprehensive settlement negotiations, the Commission and House of Philadelphia have agreed that this action should be finally resolved by entry of this Consent Decree.\\nIt is ORDERED, ADJUDGED AND DECREED: .1. This Decree resolves all claims arising out of the issues between the Commission and Defendant House of Philadelphia in this lawsuit, including without limitation, back pay, compensatory and punitive damages, injunctive relief, costs, and attorney fees. This decree is limited in its scope to matters covered explicitly herein and in particular only to the House of Philadelphia facilities in the State of Alabama. This Decree expires when House of Philadelphia has provided the relief and taken the action provided for herein, or as provided under paragraph 12 of this Decree, whichever is later. 2. Defendant and its officers, agents, employees, successors, and assigns both at the time that this Decree becomes effective and for the duration of this Decree agree to comply with Federal law and acknowledge that it is unlawful to: (a) discriminate against any employee on the basis of pregnancy or sex, (b) harass any employee based on pregnancy or sex; (c) retaliate against any employee because he or she: (i) opposes or opposed discriminatory practices nlade unlawful by Title VII; (ii) files or filed a charge of discrimination or assists, assisted,\\n2\\n\\n\\x0cparticipates, or participated in the filing of a charge of discrimination; or (iii) assists, assisted, participates or participated in an investigation or proceeding brought under the federal laws prohibiting discrimination or retaliation; and (d) alter the terms and conditions of any employee\\'s employment because of pregnancy or sex.\\nIVI0NETARY RElJEF 3. House of Philadelphia shaH pay, by certified checks, a total amount of $8,000.00 to Sharonda Griffin. Of this sum, $4,000.00 (four thousand dollars) shall be paid by House of Philadelphia to M\\'s. Griffin by October 6, 2006 and $4,000.00 (four thousand dollars) shall be paid by House of Philadelphia to Ms. Griffin by Monday, November 6, 2006. House of Philadelphia will not condition the receipt of individual relief on Ms. Griffin\\'s agree.ment to (a) maintain as confidential the tenns of this Decree, or (b) waive her statutory rights to file a charge with any federal or state anti-discrimination agency. House of Philadelphia will issue applicable United States Internal Revenue Service Forms to Ms. Griffin for all such payments on or before January 31, 2007. 4. The pa}\\'rnents provided for in paragraph 3 of this Decree shall be malled directly by House of Philadelphia\\'s attorney to Ms. Griffin\\'s attorney, Daniel A. Hannan, at Mr. Hannan\\'s business address. \\\\Vithin three (3) business days of the issuance of each cbeck, House of Philadelphia shall submit a copy of the check and related correspondence to the United States Equal Emp]oyment Opportunity Commission, Regional Attorney, Binningham District Office, 1130 22nd Street South, Suite 2000, Binningham, Alabama, 35205-2886.\\nOTHER RELIEF 5. Defendant will institute and carry out policies, practices and training at its Alabama facilities that help assure a \\\\\\\\tork environment free lrOln sex and pregnancy-based discrinlination\\n3\\n\\n\\x0cfor its employees that allow employees to raise concerns or complaints \\\\vithout retaliation about\\n\\nmatters, whether alleged~ perceived or actual, made unlawful by Title VII~ and that guide, direct\\n\\nand encourage employees to report incidents of pre~\\'11ancy and sex-based discrimination.\\n\\n6.\\n\\nDefendant will develop and adopt policies that inc1ude~ at a minimum:\\n\\na. A clear and strong commitment to a workplace free of pregnancy and sex-based\\n\\ndiscrimination;\\n\\nb. A clear and strong message of encouragement to persons who believe they have\\n\\nbeen discriminated against to come fonvard;\\n\\nc. A description of the consequences, up to and including tennination, that will be\\n\\nimposed upon violators of the policy;\\n\\nd. An assurance of non-retaliation for persons who believe they have been\\n\\ndiscriminated against and for witnesses;\\n\\ne. That discrimination on the basis of sex by anyone, including management\\n\\nofficials, supervisors, vendors, suppliers, third parties and customers, is prohibited and will not\\n\\nbe tolerated;\\n\\nf.\\n\\nAssurances that Defendant will investigate allegations of pregnancy and sex-\\n\\nbased discrimination promptly, fairly, reasonably, effectively and as confidentially as possible\\n\\nunder the circmnstances, by appropriate investigators and that appropriate corrective action and\\n\\nappropriate foHow-up will be taken by Defendant to make victims whole and to eradicate the\\n\\ndiscrimination;\\n\\ng. That infomlation will be provided each employee regarding the employee\\'s right\\n\\nto file a charge of discrimination with the EEOC, including contact telephone numbers,\\n\\nTDYITDD and addresses for the EEOC.\\n\\n4\\n\\n\\x0c7. \\\\Vithin sixty (60) days follo\\\\ving the date of entry of this Decree, and annually thereafter for the duration of this Decree, Defendant will provide training at each of its Alabama facilities which shaH explain:(l) what constitutes pregnancy and sex-based discrimination; (2) that Title\\nvn prohibits this misconduct; (3) how to prevent this misconduct; (4) to whom employees may\\ncomplain if they feel they have been subjected to this Inisconducl; and (5) that managers will be evaluated on their enforcen1ent of House of Philadelphia\\'s anti-discrimination policies. This training will also include an explanation of House of Philadelphia\\'s policies regarding pregnancy and sex-based discrimination; the importance of maintaining an environn1ent free from pregnancy and sex discrinlination; and the discipline that Inay be taken against other employees and the managers or supervisors who are found to have allowed the discrimination to occur. Pursuant to this Decree, House of Philadelphia will also conduct the same or similar training at intervals of approximately twelve (12) months during the pendency of this Decree. The following subparagraphs refer to each of the two training periods.\\na. The annual training session shall be at least two (2) hours in length, plus an additional thirty (30) minutes for questions and answers. Defendant\\'s Alabama management staff, including all managers who may investigate employee complaints shall attend the annual training sessions together with all hourly and salaried employees. The enlployees who are unable to attend the annual training session nlay watch a videotape of it.\\nb. Employees shall sign a registry when they attend the annual training session or watch the videotape of the annual training session. Defendant shall keep, for the duration of the Decree, this written record of all employees who attend the annual training session or watch it on videotape.\\n5\\n\\n\\x0cc.\\n\\nHOllse of Philadelphia win confinn in writing to the EEOC Regional Attorney\\n\\nthat this additional training has been completed in full compliance with the tenns of this Decree.\\n\\n\\\\Vithin ten (l0) days following its receipt, EEOC will confirm that this training requirement has\\n\\nbeen met or provide specific comments about how the training is not in compliance with the\\n\\nDecree. The parties would communicate to resolve any disputed issues on this training or, if\\n\\nnecessary, have the court resolve the matter.\\n\\n8. I-louse of Philadelphia will post its current EEO policies in a prominent location,\\n\\nfrequented by enlployees, at each of Defendant\\'s facilities in A1abama. These policies shaH also\\n\\nbe distributed to each current employee within thirty (30) days of the entry of the Decree, and\\n\\ndistributed to all new employees when hired.\\n\\n9. Defendant shall promptly and appropriately investigate aU complaints of pregnancy and\\n\\nsex-based discrimination. The investigation must inc1ude a finding of \\\\vhether discrimination\\n\\noccurred; a credibility assessment; interviews of all potential victims and witnesses identified;\\n\\nand concurrent notes of the investigation. Defendant shall take immediate appropriate corrective\\n\\naction to nlake discrinlination victims whole, to discipline violators, and to eradicate the\\n\\ndiscrimination. Defendant, if corrective action was required as a result of the investigation, shall\\n\\nfollow up with complainants at appropriate intervals to ensure that the discrimination does not\\n\\nreoccur. Defendant shall provide notice to EEOC of the resolutions of cOlnplaints of sex or\\n\\npregnancy-based discrimination during the pendency of this decree. That notice shall describe\\n\\nthe investigation, and the resolution of the investigation shall be provided to the EEOC within six\\n\\n(6) months of the initial complaint.\\n\\n6\\n\\n\\x0cAPPROVED A~D CONSE~TEf) TO BY:\\n\\nChiz,,e\"\":fA E4J~Jte,jfrA~,tY\\nHOtlSE OF PHILADELPH lAo\\nCENTER. INC\\n\\nAtto • for Defendant RA )\\'MONDL. BELL. ESQ.\\nBen &: Adanis. P,C.\\nPO, Box 1932 lvtohile, AL 36602 Telephone: (251) 694-9020\\n\\nAttorney for Plaintiff-Intervenor DANIEL A, HANNAN~ ESQ. 63 S. Royal Street, Suite 1109 ~fobile, Alabama 36602 Telephone; (251) 433-0051\\nATTORNEYS FOR PLAINTll1\\'F KEOS::\\n\\nC. EMANUEL SMITH Regional Attorney\\nSupervisory Trial Attorney\\nMASON D, BARRETT Senior Trial Attorney EQTJ AL El\\\\1PLOY~fENT OPPORTUNITY CO~1MISS[ON Binningham District Ofi1ce t 13() 22nd Street South,. Suite 2000 Bjrminghrun~ AL 35205~2886 Telephone: (205) 212-2047 Facsimile: (205) 212-2041\\n8\\n\\n\\x0cNOTICE\\n\\n10. Defendant will post the Notice attached as Attacl1Jllent A at each of Defendant\\'s facilities\\n\\nin Alabama. The Notice will be posted in a highly visible location, frequented by employees, for\\n\\nthe duration of this Decree. The Notice shall be posted in both English and in Spanish, and shall\\n\\nbe the same type, size, and style as Attachment A.\\n\\n11 w The parties shall bear their O\\\\\\\\\\'Il attorneys\\' fees and costs i~curred in this action up to the\\n\\ndate. of entry ofthis Decree.\\n\\nFORCE AND EFFECf\\n\\n12.\\n\\nThe duration of this Decree shall be thirty-six (36) months from ·its entry. This Court\\n\\nshall retain jurisdiction over this action for the duration of the Decree, during which the\\n\\nCommission may petition this Court for compliance with this Decree. Should the Court\\n\\ndetennine that defendant has not complied with this Decree, appropriate relief, including\\n\\nextension ofthis Decree for such period as may be necessary to remedy its non~comp1iance, may\\n\\nbe ordered. Absent extension, this Decree shall expire by its own tenns at the end of thirty-si~\\n\\n(36) months from the date of entry, without further action by the Parties.\\n\\n13. The parties agree to the entry ofthis Decree subject to final approval by the Court.\\n\\nIT IS SO ORDERED: This 9th day of January 2007.\\n\\n)\\n\\n///\\n\\n. / ) f)\\n\\n~~_/~\"\\'-~\\'-~~-~~-=~~~--r-___~/~\\'~~~/~~~(j~?\\n\\nISTI K. DuBOSE\\n\\nUNITED STATES DISTRICT JUDGE\\n\\n7\\n\\n\\x0c\\n\\n# Task: summarize\\n# Output JSON:\\n```json',\n",
       " 'input': '',\n",
       " 'output': '```json\\n{\"Summary_long\": \"On September 15, 2005, the Equal Employment Opportunity Commission (EEOC) filed suit against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. Seeking monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages), the EEOC brought suit under Title VII of the Civil Rights Act of 1964 for unlawful discrimination on the basis of sex. The EEOC also sought to recover its costs.\\\\n\\\\nVia private counsel, the employee filed a motion to intervene in the suit, which was automatically granted after the period for filing objections passed without incident. The employee brought claims under Title VII and state law and sought substantially the same relief as the EEOC, except that the complaint specifically sought reinstatement.\\\\n\\\\nEventually the parties came to a settlement agreement, which the Court (Judge Kristi K. DuBose) entered as a consent decree on Jan 10, 2009. The terms of the decree, which lasted 3 years, provided monetary and injunctive relief. The employee received $8,000, while House of Philadelphia, Inc., was required to institute and follow policies to eliminate sex discrimination and pregnancy discrimination from the workplace and to post and distribute the policies to employees.  House of Philadelphia also had to provided yearly training to its employees explaining pregnancy and sex discrimination, informing them of its illegality, and explaining how to avoid it, who to file complaints with, and that managers would be evaluated for enforcing the anti-discrimination policies. House of Philadelphia was further required to investigate complaints adequately and report the results of each investigation to the EEOC. The 3-year decree period passed without court involvement and the case is now closed.\", \"Summary_short\": \"Equal Employment Opportunity Commission brought a Title VII sex discrimination case against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. The EEOC sought monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages). The employee later intervened in the suit. The parties came to a settlement that was entered as a consent decree. The decree provided for monetary relief for the employee and subjected House of Philadelphia to a 3-year injunction. The case is now closed.\"}\\n```',\n",
       " 'history': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:14:18.168094Z",
     "iopub.status.busy": "2025-05-11T14:14:18.167408Z",
     "iopub.status.idle": "2025-05-11T14:14:23.891730Z",
     "shell.execute_reply": "2025-05-11T14:14:23.891135Z",
     "shell.execute_reply.started": "2025-05-11T14:14:18.168069Z"
    },
    "id": "DvWmfB-QPJUs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
    "\n",
    "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
    "    json.dump(train_data, dest, ensure_ascii=False, default=str)\n",
    "\n",
    "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
    "    json.dump(val_data, dest, ensure_ascii=False, default=str)\n",
    "\n",
    "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"test.json\"), \"w\", encoding=\"utf8\") as dest:\n",
    "    json.dump(test_data, dest, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoBc56BiRDLO"
   },
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMTk3i2fQKk5",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # # Configure LLaMA-Factory for the new datasets\n",
    "\n",
    "# update /kaggle/working/LLaMA-Factory/data/dataset_info.json and append\n",
    "# ```\n",
    "#    \"summarizer_finetune_train\": {\n",
    "#         \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/train.json\",\n",
    "#         \"columns\": {\n",
    "#             \"prompt\": \"instruction\",\n",
    "#             \"query\": \"input\",\n",
    "#             \"response\": \"output\",\n",
    "#             \"system\": \"system\",\n",
    "#             \"history\": \"history\"\n",
    "#         }\n",
    "#     },\n",
    "#     \"summarizer_finetune_val\": {\n",
    "#         \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/val.json\",\n",
    "#         \"columns\": {\n",
    "#             \"prompt\": \"instruction\",\n",
    "#             \"query\": \"input\",\n",
    "#             \"response\": \"output\",\n",
    "#             \"system\": \"system\",\n",
    "#             \"history\": \"history\"\n",
    "#         }\n",
    "#     }\n",
    "# ```\n",
    "\n",
    "# # https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
    "# # https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:14:23.892863Z",
     "iopub.status.busy": "2025-05-11T14:14:23.892649Z",
     "iopub.status.idle": "2025-05-11T14:14:23.908302Z",
     "shell.execute_reply": "2025-05-11T14:14:23.907796Z",
     "shell.execute_reply.started": "2025-05-11T14:14:23.892848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset_info.json with summarizer entries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os import path\n",
    "\n",
    "# Path to your dataset_info.json\n",
    "info_path = \"/kaggle/working/LLaMA-Factory/data/dataset_info.json\"\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load existing JSON\n",
    "with open('/kaggle/input/dataset-info-json/dataset_info.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Write it back (pretty-printed)\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Updated {path.basename(info_path)} with summarizer entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:14:24.440926Z",
     "iopub.status.busy": "2025-05-11T14:14:24.440723Z",
     "iopub.status.idle": "2025-05-11T14:14:24.445517Z",
     "shell.execute_reply": "2025-05-11T14:14:24.445006Z",
     "shell.execute_reply.started": "2025-05-11T14:14:24.440910Z"
    },
    "id": "TCWgFdsoQrow",
    "outputId": "4a48543d-52b6-4a9f-ef72-84ffb674a56a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/LLaMA-Factory/examples/train_lora/summarizer_finetune.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/LLaMA-Factory/examples/train_lora/summarizer_finetune.yaml\n",
    "\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 64\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "dataset: summarizer_finetune_train\n",
    "eval_dataset: summarizer_finetune_val\n",
    "template: qwen\n",
    "cutoff_len: 3500\n",
    "# max_samples: 50\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "# resume_from_checkpoint: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/checkpoint-1500\n",
    "output_dir: /kaggle/working/llm-finetuning/models/\n",
    "logging_steps: 10\n",
    "save_steps: 150\n",
    "plot_loss: true\n",
    "# overwrite_output_dir: true\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 1.0e-4\n",
    "num_train_epochs: 2.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "#error rec\n",
    "resume_from_checkpoint: false  # Explicitly disable resume\n",
    "overwrite_output_dir: true     # Clean previous checkpoints\n",
    "### eval\n",
    "# val_size: 0.1\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 100\n",
    "\n",
    "report_to: wandb\n",
    "run_name: legalx-finetune-llamafactory\n",
    "\n",
    "push_to_hub: true\n",
    "export_hub_model_id: \"masry1/legal_summarization\"\n",
    "hub_private_repo: true\n",
    "hub_strategy: checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T14:14:26.499999Z",
     "iopub.status.busy": "2025-05-11T14:14:26.499313Z",
     "iopub.status.idle": "2025-05-11T15:16:09.813284Z",
     "shell.execute_reply": "2025-05-11T15:16:09.812306Z",
     "shell.execute_reply.started": "2025-05-11T14:14:26.499966Z"
    },
    "id": "Wkigp2KPVgqU",
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 14:14:33.932953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746972874.181201     286 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746972874.250122     286 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[INFO|2025-05-11 14:14:53] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:44839\n",
      "W0511 14:14:54.967000 359 torch/distributed/run.py:793] \n",
      "W0511 14:14:54.967000 359 torch/distributed/run.py:793] *****************************************\n",
      "W0511 14:14:54.967000 359 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0511 14:14:54.967000 359 torch/distributed/run.py:793] *****************************************\n",
      "2025-05-11 14:15:00.273843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 14:15:00.285106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 14:15:00.285185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 14:15:00.285283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746972900.295235     361 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746972900.301815     361 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746972900.306227     364 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746972900.306557     363 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746972900.306645     362 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746972900.312711     364 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1746972900.313108     363 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1746972900.313177     362 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[W511 14:15:05.695385441 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "[W511 14:15:05.703682561 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "[W511 14:15:05.704093843 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "[W511 14:15:05.704181064 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "[INFO|2025-05-11 14:15:05] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-11 14:15:05] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-11 14:15:06] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-11 14:15:06] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-11 14:15:06] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\n",
      "tokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 33.4MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 18.8MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 19.2MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 38.4MB/s]\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:07,454 >> loading file chat_template.jinja from cache at None\n",
      "config.json: 100%|█████████████████████████████| 660/660 [00:00<00:00, 5.40MB/s]\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-05-11 14:15:07,798 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[rank2]:[W511 14:15:08.058932848 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank3]:[W511 14:15:08.141190847 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 14:15:08,306 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 14:15:08,307 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,351 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,351 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,351 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,352 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,352 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,352 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-05-11 14:15:08,352 >> loading file chat_template.jinja from cache at None\n",
      "[rank1]:[W511 14:15:08.262232154 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-05-11 14:15:08,688 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-11 14:15:08] llamafactory.data.loader:143 >> Loading dataset /kaggle/working/datasets/llamafactory-finetune-data/train.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 3177 examples [00:12, 260.21 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 3177/3177 [00:01<00:00, 1762\n",
      "[INFO|2025-05-11 14:15:23] llamafactory.data.loader:143 >> Loading dataset /kaggle/working/datasets/llamafactory-finetune-data/val.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 454 examples [00:01, 296.51 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 454/454 [00:00<00:00, 1737.6\n",
      "[rank0]:[W511 14:15:25.412745477 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/3177 [00:00<?, ? examples/[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:29,444 >> Token indices sequence length is longer than the specified maximum sequence length for this model (401757 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:30,272 >> Token indices sequence length is longer than the specified maximum sequence length for this model (441341 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:30,555 >> Token indices sequence length is longer than the specified maximum sequence length for this model (137216 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:30,915 >> Token indices sequence length is longer than the specified maximum sequence length for this model (278604 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:31,137 >> Token indices sequence length is longer than the specified maximum sequence length for this model (170973 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:31,347 >> Token indices sequence length is longer than the specified maximum sequence length for this model (204435 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:32,157 >> Token indices sequence length is longer than the specified maximum sequence length for this model (132951 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:32,908 >> Token indices sequence length is longer than the specified maximum sequence length for this model (181421 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:33,084 >> Token indices sequence length is longer than the specified maximum sequence length for this model (143007 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:33,354 >> Token indices sequence length is longer than the specified maximum sequence length for this model (138366 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:34,594 >> Token indices sequence length is longer than the specified maximum sequence length for this model (371849 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:36,799 >> Token indices sequence length is longer than the specified maximum sequence length for this model (270888 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:37,344 >> Token indices sequence length is longer than the specified maximum sequence length for this model (198106 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:38,971 >> Token indices sequence length is longer than the specified maximum sequence length for this model (230322 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:39,662 >> Token indices sequence length is longer than the specified maximum sequence length for this model (143257 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:15:46,627 >> Token indices sequence length is longer than the specified maximum sequence length for this model (206857 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 3177/3177 [00:34<00:00, 93.2\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 11789, 510, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 16, 315, 220, 21, 271, 687, 271, 17229, 271, 23040, 1479, 271, 22595, 1570, 271, 79440, 39477, 271, 37, 30215, 53448, 350, 271, 47, 16, 24, 271, 13, 15, 20, 271, 45, 75, 271, 301, 271, 514, 271, 30902, 3168, 328, 3656, 3012, 45, 36103, 39477, 3008, 8753, 1867, 38100, 271, 50, 3656, 3012, 45, 46260, 24211, 271, 96024, 51592, 35666, 5328, 94181, 2868, 56173, 10444, 8696, 25245, 11, 10444, 60, 77282, 11, 2279, 16398, 5586, 2308, 13, 83233, 12, 220, 15, 6, 20, 18, 64, 481, 58629, 85, 382, 2533, 921, 8696, 2916, 32, 3221, 271, 60, 68361, 3008, 14659, 1715, 32841, 43, 10842, 5863, 65992, 11, 18118, 659, 10444, 41, 75629, 54563, 969, 50814, 3976, 271, 2620, 20372, 6762, 921, 921, 60, 10444, 45, 14870, 3008, 3168, 16054, 1096, 374, 458, 1917, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 323, 10869, 358, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 24, 16, 311, 4396, 44422, 14402, 12378, 389, 279, 8037, 315, 1839, 323, 311, 3410, 8311, 15957, 311, 35116, 18194, 40396, 879, 572, 68114, 11495, 553, 1741, 12378, 659, 576, 9652, 58697, 429, 279, 87545, 14238, 15479, 2348, 35116, 18194, 40396, 1576, 315, 1059, 1839, 11, 8778, 6762, 16, 271, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 17, 315, 220, 21, 198, 41, 1511, 1637, 17625, 3580, 3567, 647, 14973, 468, 220, 16, 659, 38798, 25623, 315, 419, 7154, 374, 26881, 32449, 311, 220, 17, 23, 547, 659, 50, 659, 34, 13, 64331, 220, 19, 20, 16, 11, 220, 16, 18, 18, 16, 11, 220, 16, 18, 18, 22, 11, 220, 16, 18, 19, 18, 323, 220, 16, 18, 19, 20, 659, 1096, 1917, 374, 18630, 323, 79999, 32449, 311, 64331, 220, 22, 15, 18, 11, 220, 22, 15, 21, 955, 2376, 16, 8, 323, 320, 18, 8, 315, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 11, 438, 29911, 11, 220, 19, 17, 547, 808, 659, 34, 13, 16625, 220, 17, 15, 15, 15, 68, 12, 17, 2877, 701, 220, 19, 17, 547, 659, 50, 659, 34, 659, 16625, 220, 17, 15, 15, 15, 68, 12, 20, 955, 2376, 75, 8, 323, 320, 18, 701, 323, 11113, 220, 16, 15, 17, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 24, 16, 11, 220, 19, 17, 547, 659, 50, 727, 13, 11113, 220, 16, 24, 23, 16, 32, 659, 220, 17, 659, 576, 44422, 14402, 12378, 10491, 311, 387, 44422, 1033, 11163, 2878, 279, 28001, 315, 279, 3639, 4180, 10942, 7154, 369, 279, 16244, 10942, 315, 20623, 11, 16244, 14489, 16448, 33490, 5369, 220, 18, 659, 77282, 11, 279, 38474, 40852, 47279, 9652, 320, 1782, 330, 73750, 3975, 374, 279, 9088, 315, 279, 3639, 4180, 315, 5159, 11430, 448, 279, 8567, 11, 22845, 323, 13324, 315, 10869, 44969, 11, 323, 374, 52511, 18630, 311, 4446, 419, 1917, 553, 16625, 220, 22, 15, 21, 955, 2376, 16, 8, 323, 320, 18, 8, 315, 10869, 44969, 11, 220, 19, 17, 547, 808, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 12, 20, 955, 2376, 75, 8, 323, 320, 18, 8, 659, 220, 19, 13, 2411, 678, 9760, 3039, 11, 279, 87545, 11, 4678, 315, 19335, 5832, 11, 66695, 3489, 28607, 315, 19335, 1, 476, 279, 330, 36032, 261, 899, 702, 30878, 1012, 3730, 2562, 304, 279, 3234, 315, 20623, 323, 279, 3283, 315, 55210, 777, 11, 323, 702, 30878, 1030, 518, 3245, 220, 16, 20, 8256, 624, 17, 271, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 18, 315, 220, 21, 198, 20, 659, 2411, 678, 9760, 3039, 11, 279, 87545, 74852, 702, 30878, 1012, 458, 19136, 16634, 304, 458, 4958, 27887, 35654, 2878, 279, 7290, 315, 59037, 220, 22, 15, 16, 1883, 701, 320, 70, 8, 323, 320, 71, 8, 315, 10869, 44969, 11, 220, 19, 17, 547, 659, 50, 659, 34, 659, 16625, 220, 17, 15, 15, 15, 68, 8013, 65, 701, 320, 70, 8, 323, 320, 71, 8, 16448, 24628, 5328, 3008, 25825, 328, 220, 21, 659, 4398, 1091, 26127, 2849, 4867, 311, 279, 14898, 315, 419, 19275, 11, 35116, 18194, 40396, 12729, 264, 36795, 315, 77300, 2554, 448, 279, 9652, 61446, 26557, 315, 10869, 44969, 553, 87545, 74852, 13, 2009, 4682, 46791, 311, 279, 14898, 315, 419, 19275, 614, 1012, 40734, 13, 220, 22, 659, 8704, 518, 3245, 6527, 220, 16, 20, 11, 220, 17, 15, 15, 19, 11, 87545, 702, 16634, 304, 44422, 14402, 12378, 518, 1181, 55210, 777, 11, 20623, 11, 12481, 304, 19940, 315, 11113, 220, 22, 15, 18, 320, 64, 8, 315, 10869, 44969, 11, 220, 19, 17, 547, 659, 50, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 12, 17, 2877, 568, 758, 3953, 11, 87545, 56091, 35116, 18194, 40396, 1576, 1340, 572, 20280, 659, 220, 23, 659, 576, 2456, 315, 279, 12378, 33970, 315, 304, 14311, 220, 22, 3403, 702, 1012, 311, 35880, 533, 35116, 18194, 40396, 315, 6144, 14402, 10488, 323, 5937, 68114, 11495, 1059, 2639, 438, 458, 9364, 11, 1576, 315, 1059, 1839, 11, 8778, 659, 220, 24, 13, 576, 44422, 14402, 12378, 33970, 315, 304, 42643, 220, 22, 323, 220, 23, 3403, 1033, 46864, 16448, 18, 271, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 19, 315, 220, 21, 198, 16, 15, 13, 576, 44422, 14402, 12378, 33970, 315, 304, 42643, 220, 22, 11, 220, 23, 323, 220, 24, 3403, 1033, 2814, 448, 8641, 558, 476, 448, 53217, 84173, 311, 279, 79537, 2617, 3188, 315, 35116, 18194, 40396, 624, 6480, 14765, 4613, 31512, 5371, 434, 5288, 33023, 1154, 279, 9652, 92345, 7388, 429, 419, 7154, 6260, 32, 13, 23736, 264, 15330, 60429, 662, 65205, 279, 87545, 74852, 11, 1181, 9611, 11, 13009, 11, 74896, 11, 49912, 323, 678, 11186, 304, 4541, 20830, 476, 20239, 448, 432, 11, 504, 22570, 304, 894, 14402, 12378, 892, 59441, 389, 279, 8037, 315, 1839, 624, 33, 659, 7217, 279, 87545, 311, 43698, 323, 6777, 700, 10186, 11, 12378, 323, 7468, 892, 3410, 6144, 14402, 10488, 369, 678, 315, 1181, 8256, 15484, 315, 1839, 323, 892, 88414, 279, 6239, 315, 1181, 3267, 323, 3042, 44422, 14402, 12378, 16448, 34, 659, 7217, 279, 87545, 311, 1281, 4361, 35116, 18194, 40396, 553, 8241, 8311, 15957, 11, 304, 14713, 311, 387, 10838, 518, 9091, 11, 323, 1008, 67641, 15957, 5871, 311, 88414, 279, 6239, 315, 1181, 44422, 14402, 12378, 16448, 35, 13, 7217, 87545, 74852, 311, 1281, 4361, 35116, 18194, 40396, 553, 8241, 1059, 448, 19857, 369, 2477, 12, 992, 15705, 658, 17683, 12942, 504, 279, 44422, 14402, 12378, 7481, 304, 42643, 220, 22, 11, 220, 23, 323, 220, 24, 3403, 11, 2670, 6646, 458, 294, 198, 19, 271, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 20, 315, 220, 21, 198, 82, 2040, 287, 11, 14269, 34004, 11, 71718, 11, 30298, 11, 18210, 11, 323, 4709, 315, 44178, 315, 2272, 11, 304, 14713, 311, 387, 10838, 518, 9091, 16448, 36, 13, 7217, 87545, 74852, 311, 2291, 35116, 18194, 40396, 81432, 25129, 369, 1181, 38170, 323, 53217, 6786, 7481, 304, 42643, 220, 22, 11, 220, 23, 323, 220, 24, 3403, 11, 304, 14713, 311, 387, 10838, 518, 9091, 16448, 37, 13, 23736, 1741, 4623, 15957, 438, 279, 7154, 409, 11852, 5871, 323, 6169, 304, 279, 584, 2734, 16448, 38, 13, 17318, 279, 9652, 1181, 7049, 315, 419, 1917, 659, 619, 75629, 54563, 969, 50814, 3976, 198, 785, 9652, 7388, 264, 21234, 9091, 389, 678, 4755, 315, 2097, 9226, 553, 1181, 12181, 16448, 1061, 987, 3641, 14634, 1154, 619, 15608, 444, 13, 444, 7099, 31624, 3251, 36645, 41253, 4689, 1930, 24273, 809, 5044, 38, 3596, 43321, 32368, 3251, 31254, 325, 326, 91609, 51592, 35666, 5328, 94181, 2868, 56173, 198, 8696, 25245, 220, 16, 23, 15, 16, 330, 43, 1, 6686, 11, 451, 659, 54, 13, 6515, 11, 10922, 220, 17, 15, 20, 15, 220, 22, 198, 20, 271, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 9420, 11789, 220, 16, 12, 16, 83001, 220, 15, 24, 14, 16, 24, 14, 17, 15, 15, 20, 5755, 220, 21, 315, 220, 21, 198, 64, 96570, 642, 468, 13, 479, 435, 7253, 23997, 14017, 45272, 2650, 16289, 25294, 18412, 14352, 3234, 4716, 3034, 671, 15, 15, 17, 18, 20, 19, 220, 21, 198, 82, 14, 28827, 422, 13, 55759, 386, 35304, 422, 659, 44187, 787, 14903, 19342, 40474, 18412, 15383, 3234, 4716, 3034, 671, 17, 16, 18, 15, 220, 24, 198, 96024, 51592, 35666, 5328, 94181, 2868, 56173, 7682, 25245, 198, 16, 16, 18, 15, 17, 17, 294, 6686, 4882, 1154, 20977, 220, 17, 15, 15, 15, 35837, 11, 20623, 220, 18, 20, 17, 15, 20, 12, 17, 23, 23, 21, 56930, 549, 320, 17, 15, 20, 8, 220, 17, 16, 17, 12, 17, 15, 19, 22, 16945, 14781, 457, 549, 320, 17, 15, 20, 8, 220, 17, 16, 17, 12, 17, 15, 19, 220, 16, 198, 21, 271, 68762, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 15843, 35, 5251, 11789, 220, 24, 2385, 83001, 220, 15, 17, 14, 17, 16, 14, 17, 15, 15, 21, 5755, 220, 16, 315, 220, 21, 271, 687, 3168, 84510, 72900, 36103, 39477, 95700, 4613, 3168, 328, 3656, 3012, 45, 36103, 39477, 3008, 8753, 1867, 38100, 198, 50, 3656, 3012, 45, 46260, 24211, 271, 96024, 51592, 35666, 5328, 271, 692, 3067, 5095, 56173, 7682, 25245, 3554, 692, 692, 12, 42002, 69178, 6138, 496, 20955, 496, 21174, 15514, 11562, 624, 96394, 3008, 14659, 1715, 32841, 43, 10842, 5863, 65992, 11, 18118, 38088, 2620, 20372, 382, 692, 34, 3090, 1715, 16054, 5664, 13, 220, 15, 20, 12, 15, 20, 18, 15, 9420, 271, 692, 692, 692, 692, 692, 692, 8696, 2916, 32, 3221, 1964, 29342, 6784, 1271, 271, 16, 13, 1096, 3717, 47182, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 11, 438, 29911, 11, 323, 271, 2120, 1641, 3092, 35116, 18194, 40396, 1083, 56776, 1584, 2329, 8186, 13, 1096, 7154, 702, 28001, 916, 419, 1917, 271, 79, 1723, 27574, 311, 10869, 220, 17, 23, 547, 808, 727, 13, 64331, 16, 18, 18, 16, 323, 220, 16, 18, 19, 18, 13, 1096, 7154, 702, 67951, 28001, 916, 271, 31297, 18852, 1584, 2329, 8186, 32449, 311, 220, 17, 23, 547, 808, 727, 13, 16625, 220, 16, 18, 21, 22, 382, 17, 13, 77282, 35116, 18194, 40396, 3489, 2120, 1641, 3092, 899, 374, 264, 8778, 916, 279, 4231, 315, 93835, 320, 16, 24, 692, 18, 13, 87545, 4678, 315, 19335, 5832, 11, 4848, 13, 3489, 2620, 20372, 899, 374, 264, 26669, 3730, 2562, 304, 279, 16244, 10942, 315, 20623, 624, 1655, 678, 3039, 9760, 311, 279, 12181, 11, 87545, 1030, 36655, 476, 803, 8256, 13, 2411, 678, 3039, 9760, 311, 279, 12181, 11, 77282, 572, 19446, 553, 87545, 13, 11954, 279, 3308, 315, 77282, 748, 14402, 11, 87545, 14238, 15479, 2348, 77282, 389, 279, 8037, 315, 1059, 1839, 323, 19636, 13, 220, 22, 13, 77282, 748, 37319, 1865, 707, 309, 5269, 23325, 911, 77282, 311, 1059, 1062, 62284, 382, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 15843, 35, 5251, 11789, 220, 24, 12, 16, 83001, 220, 15, 17, 14, 17, 16, 14, 17, 15, 15, 21, 5755, 220, 17, 315, 220, 21, 198, 2120, 1641, 3092, 572, 31272, 504, 1059, 14402, 389, 6527, 753, 220, 20, 11, 220, 17, 15, 15, 19, 13, 220, 24, 13, 56311, 18852, 34408, 572, 3118, 389, 1059, 19636, 13, 220, 16, 15, 13, 758, 264, 6524, 220, 16, 78, 77282, 11, 29005, 6527, 753, 220, 20, 11, 220, 17, 15, 15, 19, 11, 16064, 13, 47769, 645, 472, 13, 32391, 1195, 11, 10560, 315, 279, 4678, 315, 19335, 5832, 11, 4848, 2572, 10982, 11, 330, 5501, 387, 25104, 17936, 697, 2473, 374, 902, 5021, 4362, 518, 4678, 297, 93, 29590, 1021, 15261, 685, 13, 23662, 311, 7413, 1055, 530, 11, 2820, 2874, 892, 358, 614, 14078, 448, 498, 13, 758, 279, 3143, 3853, 1283, 498, 614, 279, 8770, 11, 358, 686, 2908, 498, 3238, 369, 279, 2813, 1549, 13, 4615, 1537, 3238, 1899, 686, 387, 6602, 11, 6527, 220, 16, 20, 11, 220, 17, 15, 15, 19, 3263, 320, 34264, 12392, 438, 76495, 220, 16, 8, 220, 16, 16, 13, 77282, 12729, 264, 31000, 36795, 315, 77300, 2554, 448, 279, 38474, 40852, 47279, 9652, 11, 320, 36, 6760, 34, 8, 36795, 5624, 220, 16, 18, 15, 12, 17, 15, 15, 20, 12, 15, 15, 23, 15, 21, 11, 304, 476, 911, 6702, 315, 220, 17, 15, 15, 19, 13, 320, 36, 6760, 34, 36795, 12392, 438, 76495, 220, 17, 8, 220, 16, 17, 13, 77282, 3949, 264, 46316, 3454, 34907, 367, 1, 504, 279, 468, 6760, 34, 13, 320, 35, 81037, 12392, 438, 76495, 220, 18, 8, 220, 16, 18, 13, 576, 468, 6760, 34, 10897, 264, 25248, 429, 6260, 33876, 17930, 10457, 429, 89798, 8554, 572, 56091, 323, 15987, 1340, 1035, 387, 6509, 369, 312, 12, 20461, 1283, 3432, 1059, 8770, 13, 7139, 5904, 14807, 89798, 8554, 748, 19636, 572, 264, 88589, 8168, 304, 39533, 306, 748, 5480, 311, 31543, 1059, 13, 43696, 1558, 537, 1824, 279, 10982, 9055, 429, 279, 89798, 8554, 53423, 39107, 13, 358, 97126, 13276, 5240, 311, 4411, 429, 279, 89798, 8554, 572, 37026, 311, 67663, 31543, 4152, 311, 1059, 19636, 11, 304, 4634, 49741, 315, 10869, 44969, 13, 220, 16, 19, 13, 2619, 10694, 11, 389, 476, 911, 6122, 220, 16, 24, 11, 220, 17, 15, 15, 20, 11, 279, 468, 6760, 34, 12729, 264, 19275, 2348, 279, 4678, 315, 19335, 5832, 11, 4848, 13, 220, 16, 20, 13, 56311, 333, 723, 318, 974, 12729, 419, 12181, 304, 20949, 624, 5338, 37502, 1752, 52395, 220, 16, 21, 13, 77282, 2840, 38282, 973, 553, 5785, 1817, 315, 279, 87831, 18367, 315, 2097, 438, 3498, 198, 12, 17, 10452, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 15843, 35, 5251, 11789, 220, 24, 12, 16, 83001, 220, 15, 17, 14, 17, 16, 16, 17, 15, 15, 21, 5755, 220, 18, 315, 220, 21, 198, 3641, 738, 13241, 35781, 13, 220, 16, 22, 13, 87545, 14238, 15479, 2348, 77282, 304, 279, 3793, 323, 9756, 321, 908, 315, 1059, 198, 20461, 11, 518, 29858, 75366, 27037, 1576, 315, 1059, 1839, 323, 19636, 13, 220, 16, 23, 13, 1634, 264, 1102, 315, 87545, 748, 46864, 323, 44422, 6786, 11, 77282, 16256, 323, 198, 1, 2139, 107, 13022, 3968, 25, 263, 3732, 1137, 311, 7676, 14269, 6646, 437, 15691, 11, 60009, 11, 10502, 13, 2325, 812, 11, 4709, 315, 44178, 315, 2272, 11, 4709, 315, 7911, 11, 323, 4709, 315, 14402, 7567, 13, 220, 16, 24, 13, 87545, 1613, 832, 448, 8641, 558, 476, 448, 53217, 84173, 311, 77282, 748, 79537, 2617, 27433, 13, 5288, 33023, 11, 77282, 18154, 68667, 5605, 11, 279, 16652, 315, 678, 91971, 3684, 5435, 311, 419, 4925, 504, 1059, 16849, 3542, 11, 1182, 2291, 11, 4065, 2291, 11, 14239, 5269, 323, 81432, 25129, 11, 7049, 323, 30334, 527, 12436, 323, 678, 1008, 15957, 24636, 8311, 553, 419, 7154, 323, 5144, 279, 198, 15666, 37502, 1752, 52395, 220, 17, 15, 13, 77282, 51824, 553, 5785, 1817, 315, 279, 87831, 18367, 315, 2097, 438, 3498, 7225, 738, 13241, 35781, 13, 220, 17, 16, 13, 87545, 748, 6786, 3465, 323, 5144, 9756, 19684, 264, 32515, 975, 4573, 369, 77282, 13, 220, 17, 17, 13, 1634, 264, 1102, 315, 87545, 748, 46864, 323, 44422, 6786, 11, 77282, 16256, 323, 9539, 311, 7676, 14269, 6646, 323, 15691, 11, 26995, 1037, 42568, 68, 11, 10502, 94922, 11, 323, 4709, 315, 44178, 315, 2272, 13, 220, 17, 18, 13, 87545, 30432, 448, 8641, 558, 476, 448, 53217, 84173, 311, 56311, 18852, 79537, 2617, 3188, 624, 12, 18, 10452, 200, 4207, 220, 16, 25, 15, 20, 1786, 85, 12, 15, 15, 20, 18, 15, 15843, 35, 5251, 11789, 220, 24, 12, 16, 83001, 220, 15, 17, 83, 17, 16, 14, 17, 15, 15, 21, 5755, 220, 19, 315, 220, 21, 198, 26513, 33023, 11, 77282, 18154, 68667, 5605, 11, 279, 16652, 315, 678, 91971, 3684, 5435, 311, 358, 53581, 8072, 261, 504, 1059, 16849, 3542, 11, 1182, 2291, 11, 4065, 2291, 11, 14239, 5269, 323, 81432, 25129, 11, 7049, 323, 518, 777, 93, 68, 1047, 527, 12436, 323, 678, 8328, 1923, 15957, 24636, 8311, 553, 419, 7154, 323, 5144, 358, 383, 21234, 624, 25, 481, 496, 20861, 37502, 1752, 52395, 481, 220, 17, 19, 13, 77282, 51824, 553, 5785, 1817, 315, 279, 87831, 18367, 315, 2097, 438, 3498, 7225, 738, 13241, 35781, 13, 220, 17, 20, 13, 87545, 31272, 56311, 18852, 14402, 1576, 315, 1059, 1839, 323, 19636, 13, 220, 17, 21, 13, 1634, 264, 1102, 315, 87545, 748, 46864, 323, 44422, 6786, 11, 77282, 16256, 323, 9539, 311, 7676, 14269, 6646, 323, 15691, 11, 60009, 11, 10502, 94922, 11, 323, 4709, 315, 44178, 315, 2272, 13, 220, 17, 22, 13, 73594, 2236, 198, 4913, 19237, 17799, 788, 330, 1925, 6122, 220, 16, 20, 11, 220, 17, 15, 15, 20, 11, 279, 38474, 40852, 47279, 9652, 320, 36, 6760, 34, 8, 12729, 7781, 2348, 4678, 315, 19335, 11, 4848, 2572, 389, 17522, 315, 458, 9364, 879, 572, 19204, 13895, 1576, 1340, 572, 20280, 13, 58705, 32284, 323, 5811, 19931, 533, 15957, 369, 279, 9364, 320, 16169, 6955, 5557, 11, 19857, 369, 14269, 11428, 11, 323, 81432, 25129, 701, 279, 468, 6760, 34, 7117, 7781, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 369, 44422, 21240, 389, 279, 8037, 315, 1839, 13, 576, 468, 6760, 34, 1083, 16105, 311, 11731, 1181, 7049, 7110, 77, 1699, 54428, 869, 16080, 11, 279, 9364, 12729, 264, 11379, 311, 54479, 304, 279, 7781, 11, 892, 572, 9463, 11676, 1283, 279, 4168, 369, 25480, 53011, 5823, 2041, 10455, 13, 576, 9364, 7117, 8186, 1212, 10869, 44969, 323, 1584, 2329, 323, 16105, 31202, 279, 1852, 15957, 438, 279, 468, 6760, 34, 11, 3650, 429, 279, 12181, 11689, 16105, 68667, 5605, 7110, 77, 1699, 67982, 279, 9677, 3697, 311, 264, 17079, 9128, 11, 892, 279, 7154, 320, 60256, 26875, 72, 730, 13, 15687, 33, 960, 8, 10636, 438, 264, 14433, 60534, 389, 4350, 220, 16, 15, 11, 220, 17, 15, 15, 24, 13, 576, 3793, 315, 279, 60534, 11, 892, 35413, 220, 18, 1635, 11, 3897, 32284, 323, 5811, 19931, 533, 15957, 13, 576, 9364, 3949, 400, 23, 11, 15, 15, 15, 11, 1393, 4678, 315, 19335, 11, 4848, 2572, 572, 2567, 311, 43698, 323, 1795, 10186, 311, 21725, 1839, 21240, 323, 19636, 21240, 504, 279, 26368, 323, 311, 1736, 323, 16417, 279, 10186, 311, 8256, 13, 220, 4678, 315, 19335, 1083, 1030, 311, 3897, 44270, 4862, 311, 1181, 8256, 25021, 19636, 323, 1839, 21240, 11, 61925, 1105, 315, 1181, 28967, 2719, 11, 323, 25021, 1246, 311, 5648, 432, 11, 879, 311, 1034, 21171, 448, 11, 323, 429, 19680, 1035, 387, 25070, 369, 61808, 279, 7147, 1737, 85913, 10186, 13, 4678, 315, 19335, 572, 4623, 2567, 311, 19314, 21171, 48572, 323, 1895, 279, 3059, 315, 1817, 8814, 311, 279, 468, 6760, 34, 13, 576, 220, 18, 4666, 60534, 4168, 5823, 2041, 5473, 21587, 323, 279, 1142, 374, 1431, 7877, 10465, 330, 19237, 16673, 788, 330, 2993, 40852, 47279, 9652, 7117, 264, 10869, 44969, 1839, 21240, 1142, 2348, 4678, 315, 19335, 11, 4848, 2572, 389, 17522, 315, 458, 9364, 879, 572, 19204, 13895, 1576, 1340, 572, 20280, 13, 576, 468, 6760, 34, 16105, 32284, 323, 5811, 19931, 533, 15957, 369, 279, 9364, 320, 16169, 6955, 5557, 11, 19857, 369, 14269, 11428, 11, 323, 81432, 25129, 568, 576, 9364, 2937, 89448, 304, 279, 7781, 13, 576, 9677, 3697, 311, 264, 17079, 429, 572, 10636, 438, 264, 14433, 60534, 13, 576, 60534, 3897, 369, 32284, 15957, 369, 279, 9364, 323, 37026, 4678, 315, 19335, 311, 264, 220, 18, 4666, 60429, 13, 576, 1142, 374, 1431, 7877, 1189, 532, 73594, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a professional NLP data parser.\n",
      "Follow the provided `Task` by the user to generate the `Output JSON`.\n",
      "Do not generate any introduction or conclusion.<|im_end|>\n",
      "<|im_start|>user\n",
      "# Document:\n",
      "Case 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 1 of 6\n",
      "\n",
      "IN\n",
      "\n",
      "THE\n",
      "\n",
      "UNITED\n",
      "\n",
      "STATES\n",
      "\n",
      "DISTRICT\n",
      "\n",
      "FILD COUR T\n",
      "\n",
      "P19\n",
      "\n",
      ".05\n",
      "\n",
      "Nl\n",
      "\n",
      "el\n",
      "\n",
      ".s\n",
      "\n",
      "FOR THE SOUTHERN DISTRICT OF ALABAMA\n",
      "\n",
      "SOUTHERN DIVISION\n",
      "\n",
      "EQUAL EMPLOYMENT OPPORTUNITY ]\n",
      "\n",
      "COMMISSION, ]\n",
      "\n",
      "] Plaintiff, ] Civil Action No. OSS- 0'53a -~\n",
      "\n",
      "v.\n",
      "\n",
      "]\n",
      "\n",
      "]\n",
      "COMPLAINT\n",
      "\n",
      "] HOUSE OF PHILADELPHIA CENTER, INC . ]\n",
      "\n",
      "JURY TRIAL DEMAND\n",
      "\n",
      "Defendant .\n",
      "\n",
      "]\n",
      "]\n",
      "] ]\n",
      "\n",
      "NATURE OF THE ACTION This is an action under Title VII of the Civil Rights Act of 1964 and Title I of the Civil Rights Act of 1991 to correct unlawful employment practices on the basis of sex and to provide appropriate relief to Sharonda Griffin who was adversely affected by such practices . The Commission alleges that the Defendant discriminated against Sharonda Griffin because of her sex, female .\n",
      "\n",
      "1\n",
      "\n",
      "\fCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 2 of 6\n",
      "JURISDICTION AND VENU E 1 . Jurisdiction of this Court is invoked pursuant to 28 U .S .C. §§ 451, 1331, 1337, 1343 and 1345 . This action is authorized and instituted pursuant to §§ 703, 706(f)(1) and (3) of Title VII of the Civil Rights Act of 1964, as amended, 42 U.S .C. § 2000e-2(a), 42 U .S .C . § 2000e-5(f)(l) and (3), and Section 102 of the Civil Rights Act of 1991, 42 U .S.C. Section 1981A . 2 . The unlawful employment practices alleged to be unlawful were committed within the jurisdiction of the United States District Court for the Southern District of Alabama, Southern Division .\n",
      "PARTIES 3 . Plaintiff, the Equal Employment Opportunity Commission (the \"Commission\"), is the agency of the United States of America charged with the administration, interpretation and enforcement of Title VII, and is expressly authorized to bring this action by § 706(f)(1) and (3) of Title VII, 42 U.S.C. § 2000e-5(f)(l) and (3) . 4. At all relevant times, the Defendant, House of Philadelphia Center, Incorporated (\"House of Philadelphia\" or the \"Employer\") has continuously been doing business in the State of Alabama and the city of Irvington, and has continuously had at least 15 employees.\n",
      "2\n",
      "\n",
      "\fCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 3 of 6\n",
      "5 . At all relevant times, the Defendant Employer has continuously been an employer engaged in an industry affecting commerce within the meaning of Sections 701(b), (g) and (h) of Title VII, 42 U .S .C . § 2000e-(b), (g) and (h) .\n",
      "STATEMENT OF CLAIM S 6 . More than thirty days prior to the institution of this lawsuit, Sharonda Griffin filed a Charge of Discrimination with the Commission alleging violations of Title VII by Defendant Employer. All conditions precedent to the institution of this lawsuit have been fulfilled. 7 . Since at least October 15, 2004, Defendant has engaged in unlawful employment practices at its Irvington, Alabama, facility in violation of Section 703 (a) of Title VII, 42 U .S.C. § 2000e-2(a). In particular, Defendant discharged Sharonda Griffin because she was pregnant . 8 . The effect of the practices complained of in paragraph 7 above has been to deprive Sharonda Griffin of equal employment opportunities and otherwise adversely affected her status as an employee, because of her sex, female . 9. The unlawful employment practices complained of in paragraphs 7 and 8 above were intentional .\n",
      "3\n",
      "\n",
      "\fCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 4 of 6\n",
      "10. The unlawful employment practices complained of in paragraphs 7, 8 and 9 above were done with malice or with reckless indifference to the federally protected rights of Sharonda Griffin.\n",
      "PRAYER FOR RELIE F WHEREFORE , the Commission respectfully requests that this Court :\n",
      "A. Grant a permanent injunction enjoining the Defendant Employer, its officers, agents, successors, assigns and all persons in active concert or participation with it, from engaging in any employment practices which discriminate on the basis of sex.\n",
      "B . Order the Defendant to institute and carry out policies, practices and programs which provide equal employment opportunities for all of its employees regardless of sex and which eradicate the effects of its past and present unlawful employment practices .\n",
      "C . Order the Defendant to make whole Sharonda Griffin by providing appropriate relief, in amounts to be determined at trial, and other affirmative relief necessary to eradicate the effects of its unlawful employment practices .\n",
      "D. Order Defendant Employer to make whole Sharonda Griffin by providing her with compensation for non-pecuniary losses resulting from the unlawful employment practices described in paragraphs 7, 8 and 9 above, including pain an d\n",
      "4\n",
      "\n",
      "\fCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 5 of 6\n",
      "suffering, emotional distress, humiliation, isolation, depression, and loss of enjoyment of life, in amounts to be determined at trial .\n",
      "E. Order Defendant Employer to pay Sharonda Griffin punitive damages for its malicious and reckless conduct described in paragraphs 7, 8 and 9 above, in amounts to be determined at trial .\n",
      "F. Grant such further relief as the Court deems necessary and proper in the public interest .\n",
      "G. Award the Commission its costs of this action . JURY TRIAL DEMAND\n",
      "The Commission requests a jury trial on all questions of fact raised by its complaint .\n",
      "Respectfully submitted , JAMES L. LEE Deputy General Counsel GWENDOLYN YOUNG REAMS Associate General Counse l EQUAL EMPLOYMENT OPPORTUNITY\n",
      "COMMISSION 1801 \"L\" Street, N .W. Washington, DC 2050 7\n",
      "5\n",
      "\n",
      "\fCase 1:05-cv-00530-D Document 1-1 Filed 09/19/2005 Page 6 of 6\n",
      "a'rles E. G rrier CHARLES GUERRIER Regional Attorney Ohio State Bar ID #002354 6\n",
      "s/ Mason D. Barrett MASON D . BARRETT Senior Trial Attorney Colorado State Bar ID #2130 9\n",
      "EQUAL EMPLOYMENT OPPORTUNITY COMMISSION\n",
      "113022 d Street South , Suite 2000 Birmingham, Alabama 35205-2886 Telephone : (205) 212-2047 Facsimile : (205) 212-204 1\n",
      "6\n",
      "\n",
      "\f\n",
      "Case 1:05-cv-00530-KD-M Document 9-t Filed 02/21/2006 Page 1 of 6\n",
      "\n",
      "IN THE UNITED STATES DISTRICT COURT FOR THE SOUTHERN DISTRICT OF ALABAMA\n",
      "SOUTHERN DIVISION\n",
      "\n",
      "EQUAL EMPLOYMENT\n",
      "\n",
      ")\n",
      "\n",
      "OPPORTUNITY COMMISSION,\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      "- plaintiff,.\n",
      "\n",
      ".).. :-.. ,\n",
      "\n",
      "..\n",
      "\n",
      "vs.\n",
      "HOUSE OF PHILADELPHIA CENTER, INC.,\n",
      "Defendant.\n",
      "\n",
      ")\n",
      "\n",
      "CIVIL ACTION NO. 05-0530-D\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      "COMPLAINT IN INTERVENTION\n",
      "\n",
      "1. This claim arises under Title VII of the Civil Rights Act of 1964, as amended, and\n",
      "\n",
      "Plaintiff Sharonda Griffin also asserts state law claims. This Court has jurisdiction over this action\n",
      "\n",
      "pursuant to Title 28 U.S.C. §§1331 and 1343. This Court has supplemental jurisdiction over\n",
      "\n",
      "plaintiffs state law claims pursuant to 28 U.S.C. § 1367.\n",
      "\n",
      "2. Plaintiff Sharonda Griffin (\"Plaintiff\") is a female over the age of nineteen (19)\n",
      "\n",
      "3. Defendant House of Philadelphia Center, Inc. (\"Defendant\") is a corporation doing business in the Southern District of Alabama.\n",
      "At all times relevant to the complaint, Defendant had fifteen or more employees. At all times relevant to the complaint, Plaintiff was employed by Defendant. During the course of Plaintiff’s employment, Defendant discriminated against Plaintiff on the basis of her sex and pregnancy. 7. Plaintiff’s supervisor made defamatory remarks about Plaintiff to her co-workers.\n",
      "\n",
      "\fCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/21/2006 Page 2 of 6\n",
      "Plaintiff was terminated from her employment on October ! 5, 2004. 9. Plaintiffs termination was based on her pregnancy. 10. In a letter 1o Plaintiff, dated October ! 5, 2004, Ms. Mamie H. Mackey, Director of the House of Philadelphia Center, Inc., stated, \"Please be advised thai your service is no longer needed at House o~Philadeiphia. Due to persofial, health reason which I have discussed with you. In the near future after you have the baby, I will consider you working for the company again. Your last working day will be Friday, October 15, 2004\". (Letter attached as Exhibit 1) 11. Plaintiff filed a timely Charge of Discrimination with the Equal Employment Opportunity Commission, (EEOC) Charge Number 130-2005-00806, in or about November of 2004. (EEOC Charge attached as Exhibit 2) 12. Plaintiff received a\"Cause Determination\" from the EEOC. (Determination attached as Exhibit 3) 13. The EEOC issued a determination that :\n",
      "Investigation revealed that Charging Party was discharged and informed she would be considered for re-employment after having her baby. Direct evidence indicates Charging Party’s pregnancy was a motivating factor in Respondent’s decision to discharge her. Evidence does not support the stated defense that the Charging Party voluntarily resigned. I fred reasonable cause to believe that the Charging Party was subjected to discriminatory discharge due to her pregnancy, in violalion of Title VII. 14. Thereafter, on or about September 19, 2005, the EEOC filed a lawsuit against the House of Philadelphia Center, Inc. 15. Plaintifftimely filed this complaint in intervention.\n",
      "First Claim For Relief 16. Plaintiffincorporates by reference each of the foregoing allegations of fact as though\n",
      "-2-\n",
      "\n",
      "\fCase 1:05-cv-00530-KD-M Document 9-1 Filed 02/2112006 Page 3 of 6\n",
      "fully set forth herein. 17. Defendant discriminated against Plaintiff in the terms and condilions of her\n",
      "employment, at ieasl partly because of her sex and pregnancy. 18. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and\n",
      "\" ¯ \": ~:onlinues to suffer emotional painand suffering, inconvenience, mental.anguish, loss of enjoyment of life, loss of income, and loss of employment benefits. 19. Defendant acled with malice or with reckless indifference to Plaintiff’s federally protected fights. WHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to this matter from her personnel files, back pay, front pay, compensatory and punitive damages, costs and attorneys’ fees and all other relief deemed appropriate by this Court and/or the\n",
      "Second Claim For Relief 20. Plaintiff incorporates by reference each of the foregoing allegations of fact as though fully set forth herein. 21. Defendant’s conduct created and/or condoned a hostile work environment for Plaintiff. 22. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and continues to suffer emotional pain and suffering, inconvenienee, mental anguish, and loss of enjoyment of life. 23. Defendant acted with malice or with reckless indifference to Plaintiffs federally protected rights.\n",
      "-3-\n",
      "\n",
      "\fCase 1:05-cv-00530-KD-M Document 9-1 Filed 02t21/2006 Page 4 of 6\n",
      "WHEREFORE, Plaintiff demands reinstatement, the removal of all unfavorable material related to Ibis mailer from her personnel files, back pay, front pay, compensatory and punitive damages, costs and atton~eys’ fees and all olher relief deemed appropriate by this Court and/or Ihe jury.\n",
      ": -.. Third Claim For Relief - 24. Plaintiff incorporates by reference each of the foregoing allegations of fact as though fully set forth herein. 25. Defendant terminated Plaintiffs employment because of her sex and pregnancy. 26. As a result of Defendant’s intentional and unlawful conduct, Plaintiff suffered and continues to suffer emotional pain and suffering, inconvenience, mental anguish, and loss of enjoyment of life. 27.```json\n",
      "{\"Summary_long\": \"On September 15, 2005, the Equal Employment Opportunity Commission (EEOC) filed suit against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. Seeking monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages), the EEOC brought suit under Title VII of the Civil Rights Act of 1964 for unlawful discrimination on the basis of sex. The EEOC also sought to recover its costs.\\n\\nVia private counsel, the employee filed a motion to intervene in the suit, which was automatically granted after the period for filing objections passed without incident. The employee brought claims under Title VII and state law and sought substantially the same relief as the EEOC, except that the complaint specifically sought reinstatement.\\n\\nEventually the parties came to a settlement agreement, which the Court (Judge Kristi K. DuBose) entered as a consent decree on Jan 10, 2009. The terms of the decree, which lasted 3 years, provided monetary and injunctive relief. The employee received $8,000, while House of Philadelphia, Inc., was required to institute and follow policies to eliminate sex discrimination and pregnancy discrimination from the workplace and to post and distribute the policies to employees.  House of Philadelphia also had to provided yearly training to its employees explaining pregnancy and sex discrimination, informing them of its illegality, and explaining how to avoid it, who to file complaints with, and that managers would be evaluated for enforcing the anti-discrimination policies. House of Philadelphia was further required to investigate complaints adequately and report the results of each investigation to the EEOC. The 3-year decree period passed without court involvement and the case is now closed.\", \"Summary_short\": \"Equal Employment Opportunity Commission brought a Title VII sex discrimination case against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. The EEOC sought monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages). The employee later intervened in the suit. The parties came to a settlement that was entered as a consent decree. The decree provided for monetary relief for the employee and subjected House of Philadelphia to a 3-year injunction. The case is now closed.\"}\n",
      "```<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 19237, 17799, 788, 330, 1925, 6122, 220, 16, 20, 11, 220, 17, 15, 15, 20, 11, 279, 38474, 40852, 47279, 9652, 320, 36, 6760, 34, 8, 12729, 7781, 2348, 4678, 315, 19335, 11, 4848, 2572, 389, 17522, 315, 458, 9364, 879, 572, 19204, 13895, 1576, 1340, 572, 20280, 13, 58705, 32284, 323, 5811, 19931, 533, 15957, 369, 279, 9364, 320, 16169, 6955, 5557, 11, 19857, 369, 14269, 11428, 11, 323, 81432, 25129, 701, 279, 468, 6760, 34, 7117, 7781, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 369, 44422, 21240, 389, 279, 8037, 315, 1839, 13, 576, 468, 6760, 34, 1083, 16105, 311, 11731, 1181, 7049, 7110, 77, 1699, 54428, 869, 16080, 11, 279, 9364, 12729, 264, 11379, 311, 54479, 304, 279, 7781, 11, 892, 572, 9463, 11676, 1283, 279, 4168, 369, 25480, 53011, 5823, 2041, 10455, 13, 576, 9364, 7117, 8186, 1212, 10869, 44969, 323, 1584, 2329, 323, 16105, 31202, 279, 1852, 15957, 438, 279, 468, 6760, 34, 11, 3650, 429, 279, 12181, 11689, 16105, 68667, 5605, 7110, 77, 1699, 67982, 279, 9677, 3697, 311, 264, 17079, 9128, 11, 892, 279, 7154, 320, 60256, 26875, 72, 730, 13, 15687, 33, 960, 8, 10636, 438, 264, 14433, 60534, 389, 4350, 220, 16, 15, 11, 220, 17, 15, 15, 24, 13, 576, 3793, 315, 279, 60534, 11, 892, 35413, 220, 18, 1635, 11, 3897, 32284, 323, 5811, 19931, 533, 15957, 13, 576, 9364, 3949, 400, 23, 11, 15, 15, 15, 11, 1393, 4678, 315, 19335, 11, 4848, 2572, 572, 2567, 311, 43698, 323, 1795, 10186, 311, 21725, 1839, 21240, 323, 19636, 21240, 504, 279, 26368, 323, 311, 1736, 323, 16417, 279, 10186, 311, 8256, 13, 220, 4678, 315, 19335, 1083, 1030, 311, 3897, 44270, 4862, 311, 1181, 8256, 25021, 19636, 323, 1839, 21240, 11, 61925, 1105, 315, 1181, 28967, 2719, 11, 323, 25021, 1246, 311, 5648, 432, 11, 879, 311, 1034, 21171, 448, 11, 323, 429, 19680, 1035, 387, 25070, 369, 61808, 279, 7147, 1737, 85913, 10186, 13, 4678, 315, 19335, 572, 4623, 2567, 311, 19314, 21171, 48572, 323, 1895, 279, 3059, 315, 1817, 8814, 311, 279, 468, 6760, 34, 13, 576, 220, 18, 4666, 60534, 4168, 5823, 2041, 5473, 21587, 323, 279, 1142, 374, 1431, 7877, 10465, 330, 19237, 16673, 788, 330, 2993, 40852, 47279, 9652, 7117, 264, 10869, 44969, 1839, 21240, 1142, 2348, 4678, 315, 19335, 11, 4848, 2572, 389, 17522, 315, 458, 9364, 879, 572, 19204, 13895, 1576, 1340, 572, 20280, 13, 576, 468, 6760, 34, 16105, 32284, 323, 5811, 19931, 533, 15957, 369, 279, 9364, 320, 16169, 6955, 5557, 11, 19857, 369, 14269, 11428, 11, 323, 81432, 25129, 568, 576, 9364, 2937, 89448, 304, 279, 7781, 13, 576, 9677, 3697, 311, 264, 17079, 429, 572, 10636, 438, 264, 14433, 60534, 13, 576, 60534, 3897, 369, 32284, 15957, 369, 279, 9364, 323, 37026, 4678, 315, 19335, 311, 264, 220, 18, 4666, 60429, 13, 576, 1142, 374, 1431, 7877, 1189, 532, 73594, 151645, 198]\n",
      "labels:\n",
      "```json\n",
      "{\"Summary_long\": \"On September 15, 2005, the Equal Employment Opportunity Commission (EEOC) filed suit against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. Seeking monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages), the EEOC brought suit under Title VII of the Civil Rights Act of 1964 for unlawful discrimination on the basis of sex. The EEOC also sought to recover its costs.\\n\\nVia private counsel, the employee filed a motion to intervene in the suit, which was automatically granted after the period for filing objections passed without incident. The employee brought claims under Title VII and state law and sought substantially the same relief as the EEOC, except that the complaint specifically sought reinstatement.\\n\\nEventually the parties came to a settlement agreement, which the Court (Judge Kristi K. DuBose) entered as a consent decree on Jan 10, 2009. The terms of the decree, which lasted 3 years, provided monetary and injunctive relief. The employee received $8,000, while House of Philadelphia, Inc., was required to institute and follow policies to eliminate sex discrimination and pregnancy discrimination from the workplace and to post and distribute the policies to employees.  House of Philadelphia also had to provided yearly training to its employees explaining pregnancy and sex discrimination, informing them of its illegality, and explaining how to avoid it, who to file complaints with, and that managers would be evaluated for enforcing the anti-discrimination policies. House of Philadelphia was further required to investigate complaints adequately and report the results of each investigation to the EEOC. The 3-year decree period passed without court involvement and the case is now closed.\", \"Summary_short\": \"Equal Employment Opportunity Commission brought a Title VII sex discrimination case against House of Philadelphia, Inc., on behalf of an employee who was allegedly fired because she was pregnant. The EEOC sought monetary and injunctive relief for the employee (including economic damage, compensation for emotional harm, and punitive damages). The employee later intervened in the suit. The parties came to a settlement that was entered as a consent decree. The decree provided for monetary relief for the employee and subjected House of Philadelphia to a 3-year injunction. The case is now closed.\"}\n",
      "```<|im_end|>\n",
      "\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/454 [00:00<?, ? examples/s[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:02,853 >> Token indices sequence length is longer than the specified maximum sequence length for this model (137373 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:04,104 >> Token indices sequence length is longer than the specified maximum sequence length for this model (148661 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on dataset (num_proc=16):   6%| | 29/454 [00:03<00:49,  8.64 e[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:04,797 >> Token indices sequence length is longer than the specified maximum sequence length for this model (152697 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:04,840 >> Token indices sequence length is longer than the specified maximum sequence length for this model (132776 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:05,095 >> Token indices sequence length is longer than the specified maximum sequence length for this model (151029 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:05,207 >> Token indices sequence length is longer than the specified maximum sequence length for this model (242060 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:05,359 >> Token indices sequence length is longer than the specified maximum sequence length for this model (236242 > 131072). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:05,364 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1122193 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on dataset (num_proc=16):  13%|▏| 57/454 [00:04<00:29, 13.64 e[WARNING|tokenization_utils_base.py:3936] 2025-05-11 14:16:05,668 >> Token indices sequence length is longer than the specified maximum sequence length for this model (260640 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 454/454 [00:07<00:00, 63.29 \n",
      "eval example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 11789, 510, 2665, 220, 16, 198, 35245, 48740, 220, 17, 15, 15, 18, 547, 808, 13, 36103, 13, 18572, 13, 393, 867, 1808, 11860, 220, 18, 15, 18, 15, 2738, 547, 808, 13, 10942, 7154, 59667, 2738, 17230, 8748, 4654, 315, 1096, 11789, 198, 50, 2034, 1093, 82651, 1718, 11, 369, 11675, 323, 678, 1008, 29193, 30083, 11, 77282, 11, 348, 13, 7369, 30289, 26465, 47, 2572, 87545, 624, 4207, 2308, 13, 220, 15, 18, 12, 15, 19, 17, 16, 7658, 53, 13002, 11278, 8485, 84510, 72900, 36103, 39477, 95700, 4613, 3168, 65090, 13660, 36103, 39477, 3008, 198, 79470, 6791, 40, 11, 65090, 13660, 46260, 24211, 220, 17, 15, 15, 18, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 220, 18, 15, 18, 15, 26, 220, 17, 15, 15, 19, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 444, 3257, 1637, 220, 24, 16, 23, 16, 198, 32227, 220, 17, 18, 11, 220, 17, 15, 15, 19, 67438, 198, 21033, 10065, 67719, 35768, 84347, 5146, 10039, 57013, 25, 547, 808, 13, 10942, 7154, 25, 26562, 1141, 8, 356, 5044, 29365, 25, 508, 334, 16, 60, 46037, 5191, 330, 50, 398, 1, 7801, 11, 15640, 13, 11418, 671, 220, 18, 18, 21, 16, 22, 11, 7937, 619, 13, 19822, 75, 1515, 46374, 671, 220, 16, 24, 15, 23, 19, 11, 576, 90736, 7801, 37625, 40474, 70408, 11, 393, 727, 2572, 220, 23, 15, 17, 36676, 11, 220, 22, 339, 16581, 11, 20148, 4311, 11, 11418, 220, 21, 19, 16, 15, 20, 11, 220, 23, 16, 21, 12, 19, 22, 17, 12, 21, 23, 15, 15, 11, 220, 23, 16, 21, 12, 19, 22, 17, 12, 21, 23, 15, 20, 3463, 14781, 457, 323, 41807, 362, 13, 39261, 46374, 671, 220, 16, 17, 18, 20, 15, 11, 69380, 1270, 11, 39261, 609, 70536, 258, 11, 393, 727, 2572, 220, 19, 16, 17, 16, 467, 13, 220, 23, 18, 6498, 794, 2572, 3360, 13, 220, 17, 20, 21, 11, 70949, 24069, 11, 46374, 220, 21, 21, 17, 15, 23, 11, 220, 24, 16, 18, 12, 24, 15, 16, 12, 15, 20, 15, 15, 11, 220, 24, 16, 18, 12, 24, 15, 16, 12, 15, 19, 16, 24, 3463, 14781, 457, 323, 22388, 422, 13, 15696, 2576, 564, 672, 11418, 671, 220, 18, 17, 23, 21, 21, 11, 15696, 2576, 564, 672, 609, 73076, 11, 30871, 11, 220, 18, 16, 17, 4312, 220, 23, 339, 6686, 11, 20148, 4311, 11, 11418, 220, 21, 19, 16, 15, 20, 11, 220, 23, 16, 21, 12, 17, 17, 16, 12, 17, 20, 20, 20, 11, 220, 23, 16, 21, 12, 17, 17, 16, 12, 23, 22, 21, 18, 3463, 14781, 457, 13, 41285, 868, 3944, 9394, 4613, 96966, 3221, 29035, 50, 13, 48573, 25, 34813, 6769, 44162, 27939, 16054, 7682, 2916, 32, 3221, 15762, 25, 60974, 16, 60, 77282, 29933, 12648, 287, 12434, 419, 1142, 389, 17522, 315, 11675, 323, 678, 1008, 3198, 19446, 553, 7369, 30289, 21863, 13, 3489, 828, 30289, 899, 879, 614, 1012, 14238, 15479, 2348, 553, 862, 19136, 11, 7369, 30289, 11, 323, 1181, 36406, 5110, 11, 323, 1035, 1473, 279, 7154, 438, 11017, 25, 358, 13, 38798, 25623, 11, 68360, 11, 323, 59905, 198, 16, 13, 77282, 29933, 12648, 287, 374, 264, 21860, 315, 279, 1584, 315, 24378, 11, 572, 19446, 553, 87545, 11, 323, 702, 12729, 458, 468, 6760, 34, 12181, 323, 12180, 279, 1290, 311, 33772, 13, 3496, 12392, 40425, 320, 67497, 438, 330, 840, 65817, 362, 899, 323, 1290, 4686, 1331, 361, 6524, 320, 67497, 438, 330, 840, 65817, 425, 38609, 17, 13, 7369, 30289, 50142, 77282, 323, 9037, 508, 334, 17, 60, 315, 1008, 3198, 1075, 1059, 13, 7369, 30289, 374, 264, 21860, 315, 279, 5302, 315, 1532, 271, 200, 17, 15, 15, 18, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 220, 18, 15, 18, 15, 11, 353, 16, 26, 220, 17, 15, 15, 19, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 444, 3257, 1637, 220, 24, 16, 23, 16, 11, 3070, 17, 271, 2665, 220, 17, 271, 98977, 1380, 432, 374, 31662, 323, 315, 1532, 15849, 1380, 1181, 12435, 1992, 315, 2562, 374, 7407, 26, 323, 646, 387, 10223, 448, 2473, 315, 1882, 518, 576, 13034, 8188, 11, 4848, 2572, 220, 20, 16, 20, 328, 13, 20148, 21294, 2572, 2014, 375, 4554, 11, 46374, 220, 21, 21, 21, 15, 18, 624, 18, 13, 362, 295, 3376, 21674, 8188, 3489, 14542, 9726, 7733, 899, 572, 264, 2820, 72531, 369, 77282, 323, 5144, 7369, 30289, 624, 33836, 17, 60, 220, 19, 13, 68360, 374, 6169, 304, 419, 7154, 438, 87545, 1558, 2562, 304, 419, 30652, 9290, 323, 702, 8900, 476, 3897, 2820, 8113, 311, 1181, 8256, 304, 419, 30652, 9290, 13, 220, 17, 23, 547, 808, 727, 13, 16625, 220, 16, 18, 24, 16, 320, 65, 8, 323, 320, 66, 1215, 220, 19, 17, 547, 808, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 220, 20, 955, 2376, 18, 4292, 20, 13, 1096, 7154, 702, 3832, 4925, 28001, 3118, 389, 6775, 3405, 28001, 11, 220, 17, 23, 547, 808, 727, 13, 16625, 220, 16, 18, 18, 16, 11, 323, 220, 19, 17, 547, 808, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 481, 220, 20, 955, 2376, 18, 4292, 32, 13, 21517, 315, 50098, 198, 21, 13, 76075, 23221, 5435, 311, 38556, 374, 38976, 9761, 369, 2953, 11, 714, 537, 369, 3198, 13, 76075, 71254, 11, 892, 374, 1483, 1172, 553, 3198, 11, 374, 264, 6770, 22091, 23221, 5435, 311, 38556, 26, 11689, 11, 432, 27934, 19636, 369, 3198, 879, 6426, 311, 653, 773, 553, 80093, 3363, 13, 77282, 11, 389, 508, 334, 18, 60, 17522, 315, 11675, 323, 678, 3800, 29193, 30083, 11, 12434, 419, 536, 1917, 2348, 7369, 30289, 369, 1839, 21240, 304, 19940, 315, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 11, 220, 19, 17, 547, 808, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 1842, 12981, 13, 323, 279, 82527, 77300, 2554, 3218, 320, 47, 6352, 701, 220, 19, 17, 547, 808, 727, 13, 16625, 220, 17, 15, 15, 15, 68, 5969, 8, 369, 10004, 323, 5144, 8241, 2820, 8113, 429, 1558, 537, 3421, 22091, 44836, 417, 1886, 369, 3198, 5435, 311, 38556, 1393, 18202, 1839, 13904, 22091, 23221, 369, 2953, 11, 1741, 438, 30478, 624, 22, 13, 7369, 30289, 594, 41208, 315, 22091, 71254, 702, 458, 30859, 84029, 5421, 389, 16064, 13, 12648, 287, 323, 1008, 3613, 315, 279, 10981, 536, 13, 9211, 22091, 44836, 417, 1886, 525, 2500, 369, 990, 1172, 553, 3198, 11, 7369, 30289, 594, 7901, 311, 3410, 10191, 369, 22091, 71254, 8437, 1181, 8778, 8256, 311, 5157, 1948, 12515, 862, 1828, 700, 8668, 98336, 22091, 7049, 11, 476, 18043, 279, 6961, 11, 14269, 323, 5896, 7049, 315, 458, 83845, 7295, 19636, 624, 23, 13, 1634, 264, 1102, 315, 7369, 30289, 5480, 311, 21687, 44836, 417, 1886, 504, 1181, 7567, 3119, 11, 16064, 13, 12648, 287, 323, 1008, 3613, 315, 279, 10981, 536, 525, 1660, 14238, 15479, 2348, 304, 279, 60974, 18, 60, 3793, 323, 4682, 315, 14402, 11, 508, 334, 19, 60, 892, 5646, 22567, 315, 7567, 1212, 54807, 8760, 7468, 11, 1576, 315, 862, 4650, 369, 19636, 13, 1096, 50313, 10869, 44969, 624, 33, 13, 77282, 29933, 12648, 287, 198, 24, 13, 77282, 29933, 12648, 287, 702, 1012, 19446, 553, 7369, 30289, 389, 264, 2480, 7246, 8037, 2474, 5534, 220, 16, 17, 11, 220, 16, 24, 24, 20, 13, 1634, 949, 315, 279, 3793, 323, 4682, 315, 1059, 14402, 11, 16064, 13, 12648, 287, 21189, 2820, 8113, 10191, 11, 2670, 10191, 315, 22091, 10975, 323, 7611, 13, 16064, 13, 12648, 287, 913, 288, 389, 1059, 1828, 17522, 323, 438, 264, 18239, 315, 279, 10981, 536, 315, 8256, 879, 525, 14238, 15479, 2348, 553, 279, 41208, 315, 71254, 504, 7369, 30289, 594, 8760, 3119, 624, 16, 15, 13, 16064, 13, 12648, 287, 572, 11, 518, 678, 3039, 9760, 311, 419, 5240, 315, 1917, 11, 264, 5220, 315, 1682, 87350, 4231, 879, 572, 11658, 448, 279, 26248, 315, 458, 34921, 476, 83845, 7295, 19636, 13, 16064, 13, 12648, 287, 11223, 429, 1059, 22091, 2820, 2453, 8760, 3410, 476, 95160, 1059, 369, 279, 2783, 315, 7194, 2524, 13, 6252, 7388, 1033, 14820, 13, 3719, 38642, 11, 16064, 13, 12648, 287, 9498, 429, 2953, 1033, 3897, 323, 3949, 3119, 10191, 315, 264, 2480, 2088, 315, 4682, 429, 646, 387, 11758, 1526, 22091, 23221, 624, 16, 16, 13, 1913, 6156, 220, 17, 21, 11, 220, 17, 15, 15, 17, 11, 16064, 13, 12648, 287, 12729, 264, 6757, 448, 279, 468, 6760, 34, 518, 1181, 20148, 4311, 12030, 8246, 508, 334, 20, 60, 304, 20148, 4311, 11, 20148, 61446, 429, 7369, 30289, 594, 7901, 311, 3410, 1059, 448, 2820, 8113, 10191, 369, 22091, 44836, 417, 1886, 41575, 44422, 21240, 389, 279, 8037, 315, 1839, 624, 16, 17, 13, 1634, 264, 1102, 11, 16064, 13, 12648, 287, 3949, 264, 5480, 323, 264, 1290, 4686, 1331, 361, 6524, 504, 279, 468, 6760, 34, 320, 77601, 315, 892, 525, 12392, 1059, 11023, 438, 330, 840, 65817, 362, 1, 323, 76495, 425, 80984, 200, 17, 15, 15, 18, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 220, 18, 15, 18, 15, 11, 353, 18, 26, 220, 17, 15, 15, 19, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 444, 3257, 1637, 220, 24, 16, 23, 16, 11, 3070, 20, 271, 2665, 220, 18, 271, 33836, 19, 60, 220, 16, 18, 13, 1913, 6702, 220, 17, 24, 11, 220, 17, 15, 15, 17, 11, 279, 468, 6760, 34, 10897, 264, 330, 35, 81037, 1, 311, 77282, 29933, 12648, 287, 11, 304, 892, 432, 6509, 7369, 30289, 594, 8760, 3119, 323, 19941, 429, 510, 52570, 275, 374, 279, 9652, 594, 2309, 429, 279, 855, 12009, 3901, 220, 17, 15, 15, 17, 41208, 50313, 2176, 10869, 44969, 323, 279, 393, 6352, 11, 2474, 279, 61182, 3421, 22091, 44836, 417, 1886, 15484, 315, 862, 10602, 7428, 13, 76075, 44836, 417, 1886, 525, 2500, 1172, 369, 3198, 13, 1634, 264, 1102, 11, 39533, 306, 594, 855, 12009, 3901, 220, 17, 15, 15, 17, 4842, 374, 11, 553, 7271, 11, 264, 1839, 5980, 41208, 1112, 2132, 7952, 429, 39533, 306, 594, 1482, 3119, 41229, 536, 9606, 44836, 417, 1886, 1483, 369, 7194, 2524, 9895, 553, 22703, 862, 23390, 21063, 553, 8072, 13, 1096, 1083, 10868, 264, 1839, 5980, 29196, 323, 50313, 279, 61182, 13, 55669, 369, 22091, 44836, 417, 1886, 1969, 387, 65599, 304, 279, 1852, 11566, 438, 421, 279, 5220, 11, 476, 894, 9364, 11, 508, 334, 21, 60, 16105, 1008, 70023, 476, 2820, 13404, 3516, 624, 16, 19, 13, 7369, 30289, 594, 43151, 311, 3410, 279, 1852, 7567, 311, 2176, 2953, 323, 3198, 702, 8881, 458, 6955, 66550, 389, 16064, 13, 12648, 287, 323, 1008, 3613, 315, 279, 10981, 536, 429, 8593, 8256, 525, 537, 2567, 311, 45653, 624, 5543, 13, 7369, 30289, 594, 6267, 9680, 198, 16, 20, 13, 1634, 3793, 323, 4682, 315, 862, 14402, 11, 7369, 30289, 8900, 16064, 13, 12648, 287, 279, 6638, 311, 51780, 304, 825, 315, 2326, 2820, 6649, 320, 20485, 568, 576, 2326, 2606, 1033, 25, 3877, 315, 5362, 320, 17456, 8, 2999, 26, 45460, 2263, 336, 77, 487, 2999, 320, 47, 2045, 1215, 323, 6267, 38206, 20395, 320, 39, 10531, 568, 576, 26494, 8900, 279, 8426, 2188, 315, 7567, 13, 576, 393, 2045, 2999, 572, 1172, 8900, 311, 1846, 8256, 879, 12163, 4889, 279, 26494, 2473, 3082, 11, 714, 5420, 6589, 315, 34844, 323, 70011, 1283, 279, 77812, 572, 2270, 13, 576, 472, 10531, 2999, 8789, 1030, 902, 36810, 20410, 311, 3367, 11, 264, 2613, 1062, 72443, 714, 2567, 13026, 311, 1490, 264, 10668, 304, 279, 472, 10531, 3922, 624, 16, 21, 13, 28800, 2480, 7246, 64095, 8256, 525, 17013, 369, 10191, 518, 279, 2813, 594, 20284, 389, 279, 1156, 1899, 315, 279, 2254, 304, 892, 279, 9364, 1619, 1735, 4743, 3951, 315, 9374, 35442, 5362, 448, 7369, 30289, 13, 28800, 2480, 7246, 8256, 525, 537, 2567, 311, 16792, 311, 5258, 508, 334, 22, 60, 9680, 7567, 11, 714, 448, 279, 472, 10531, 2999, 11, 807, 1231, 387, 2567, 311, 653, 773, 624, 33836, 20, 60, 220, 16, 22, 13, 16064, 13, 12648, 287, 16290, 311, 1896, 9423, 315, 279, 393, 2045, 2999, 624, 16, 23, 13, 576, 9680, 1083, 5707, 369, 264, 76075, 25109, 66350, 9680, 892, 572, 36918, 553, 8755, 377, 5251, 291, 1015, 320, 77793, 9680, 568, 576, 25109, 9680, 3897, 5506, 7567, 311, 13026, 304, 279, 26494, 323, 393, 2045, 2606, 13, 758, 4586, 11, 279, 25109, 9680, 9761, 678, 10975, 3897, 553, 264, 89294, 518, 279, 27279, 594, 1973, 7241, 11689, 27444, 13, 576, 25109, 9680, 11689, 27444, 10545, 64, 60, 3834, 10975, 476, 29910, 1483, 21063, 369, 7194, 2524, 11, 2670, 20655, 44836, 417, 1886, 11, 502, 613, 550, 11, 11756, 4122, 11, 7611, 11, 62184, 476, 64146, 10040, 16, 24, 13, 17715, 18202, 1008, 96320, 6457, 3516, 323, 64735, 11, 13866, 279, 9680, 6329, 279, 25109, 9680, 3897, 369, 22091, 10975, 323, 5144, 7611, 1483, 553, 50564, 323, 1008, 3198, 29193, 30083, 311, 5358, 19636, 624, 17, 15, 13, 1416, 42002, 6116, 20280, 11, 4764, 11, 279, 9680, 1035, 614, 9761, 279, 7049, 315, 2987, 458, 20107, 476, 14354, 279, 19636, 311, 4647, 481, 53684, 1340, 14554, 311, 653, 624, 22615, 13, 90971, 311, 16064, 13, 12648, 287, 323, 6944, 3228, 16954, 198, 17, 16, 13, 1634, 264, 2118, 323, 21542, 3426, 1102, 315, 279, 7369, 30289, 9680, 594, 7901, 311, 3421, 71254, 508, 334, 23, 60, 311, 5358, 19636, 11, 16064, 13, 12648, 287, 323, 1008, 10981, 3613, 315, 279, 536, 1033, 2567, 311, 2291, 369, 862, 15088, 476, 38345, 44836, 417, 1886, 389, 458, 700, 8668, 98336, 8037, 11, 476, 5214, 72343, 19636, 13, 8909, 1657, 9314, 3198, 315, 41789, 4231, 11, 71254, 374, 279, 1172, 22091, 5506, 16064, 13, 12648, 287, 1483, 389, 264, 5792, 323, 12966, 8037, 624, 17, 17, 13, 1913, 1995, 323, 16396, 11, 7369, 30289, 50142, 11499, 315, 3198, 315, 41789, 4231, 879, 990, 22091, 71254, 382, 200, 17, 15, 15, 18, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 220, 18, 15, 18, 15, 11, 353, 20, 26, 220, 17, 15, 15, 19, 547, 808, 13, 27604, 13, 62279, 13, 393, 26060, 819, 444, 3257, 1637, 220, 24, 16, 23, 16, 11, 3070, 23, 271, 2665, 220, 19, 271, 17, 18, 13, 1416, 71254, 1033, 11758, 389, 458, 6144, 8037, 448, 1008, 64735, 1212, 279, 9680, 323, 279, 25109, 9680, 11, 16064, 13, 12648, 287, 323, 1008, 10981, 536, 3613, 1035, 614, 7171, 264, 3347, 1062, 70088, 11, 476, 902, 1062, 70088, 518, 678, 11, 369, 13954, 476, 6741, 11494, 11, 220, 18, 19, 1899, 8149, 11, 315, 71254, 624, 33836, 21, 60, 16824, 13, 434, 11944, 23752, 198, 17, 19, 13, 1752, 916, 26127, 1635, 315, 862, 6305, 11, 3198, 614, 279, 23275, 4650, 369, 19636, 73594, 2236, 198, 4913, 19237, 17799, 788, 330, 1925, 6058, 220, 17, 18, 11, 220, 17, 15, 15, 19, 11, 77282, 12729, 458, 29911, 12181, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 323, 279, 82527, 77300, 2554, 3218, 11, 220, 19, 17, 547, 808, 727, 13, 1124, 84, 15, 15, 64, 22, 220, 17, 15, 15, 15, 68, 1842, 12981, 13, 323, 220, 17, 15, 15, 15, 68, 5969, 701, 2348, 7369, 30289, 13034, 304, 279, 3639, 4180, 10942, 7154, 369, 279, 10867, 10942, 315, 24378, 13, 220, 576, 42002, 11, 15251, 553, 869, 16080, 11, 572, 264, 4741, 7369, 30289, 9364, 323, 4588, 279, 7154, 369, 9445, 5269, 323, 5811, 19931, 533, 15957, 11, 438, 1632, 438, 25129, 11, 61446, 429, 7369, 30289, 594, 2820, 8113, 4842, 14238, 15479, 2348, 3198, 13, 220, 44763, 11, 279, 42002, 683, 2883, 429, 279, 25799, 594, 2820, 8113, 3119, 11, 892, 1521, 537, 3410, 22091, 44836, 417, 1886, 320, 27713, 2524, 8, 1573, 220, 17, 15, 15, 17, 323, 1172, 1526, 279, 8072, 1283, 220, 17, 15, 15, 17, 11, 33421, 8778, 8256, 6, 8267, 3188, 7110, 77, 1699, 785, 1917, 13214, 3855, 304, 279, 547, 808, 13, 10942, 7154, 369, 279, 10942, 315, 20148, 11, 714, 572, 22409, 311, 24378, 389, 3217, 220, 16, 17, 11, 220, 17, 15, 15, 18, 13, 220, 1096, 374, 279, 2400, 389, 892, 279, 653, 3996, 12033, 13, 220, 24805, 11, 1052, 1033, 1378, 50564, 389, 279, 1142, 11, 714, 825, 12226, 700, 315, 279, 38625, 13, 220, 14301, 11, 264, 3175, 42002, 11691, 13241, 279, 1142, 7110, 77, 1699, 1925, 6122, 220, 18, 11, 220, 17, 15, 15, 19, 11, 279, 7154, 320, 60256, 59927, 8, 14820, 279, 42002, 594, 11379, 369, 536, 27606, 13, 220, 758, 279, 9459, 11, 279, 7154, 19460, 14078, 279, 8502, 369, 264, 536, 25059, 19275, 11, 3650, 311, 1459, 700, 14260, 487, 13, 220, 576, 42002, 11, 2474, 279, 7781, 6009, 11, 1030, 10497, 4633, 7194, 2524, 323, 572, 10887, 311, 633, 20280, 13, 220, 15277, 11, 279, 7154, 14275, 429, 264, 536, 1410, 537, 387, 22909, 2041, 264, 536, 18239, 879, 1035, 614, 458, 14195, 2734, 304, 279, 66697, 15193, 15957, 13, 220, 576, 42002, 4829, 311, 11731, 1059, 3267, 52769, 389, 279, 30931, 44836, 417, 1886, 13, 220, 576, 10973, 594, 7506, 2390, 3697, 8630, 279, 1142, 438, 458, 1079, 58244, 13, 220, 4636, 264, 34206, 18335, 4168, 11, 279, 7154, 1865, 2441, 17408, 7110, 77, 1699, 1925, 5534, 220, 22, 11, 220, 17, 15, 15, 21, 11, 279, 7154, 320, 60256, 59927, 8, 11676, 279, 42002, 594, 11379, 369, 536, 27606, 438, 311, 279, 25129, 304, 279, 12181, 13, 220, 576, 536, 572, 38956, 315, 8778, 8256, 315, 7369, 30289, 11, 879, 7171, 369, 862, 1828, 22091, 71254, 504, 279, 12713, 315, 6527, 220, 18, 16, 11, 220, 17, 15, 15, 16, 311, 5768, 220, 17, 11, 220, 17, 15, 15, 17, 13, 220, 576, 7154, 1521, 537, 92230, 279, 8186, 369, 5811, 19931, 533, 476, 9445, 5269, 15957, 1576, 438, 315, 220, 17, 15, 15, 17, 11, 279, 2820, 8113, 9109, 9761, 7194, 2524, 1526, 8072, 1973, 13, 5976, 279, 2783, 315, 279, 42002, 594, 4650, 13351, 572, 12040, 2613, 11, 400, 21, 23, 13, 15, 22, 11, 279, 536, 1917, 4650, 304, 279, 38625, 11, 2661, 7369, 30289, 594, 1379, 1865, 2176, 9677, 4367, 76749, 13, 220, 10548, 311, 279, 653, 3996, 11, 13866, 3108, 572, 2213, 448, 419, 5480, 13, 220, 576, 42002, 4829, 279, 4168, 315, 882, 369, 25129, 8186, 311, 387, 11577, 11, 323, 279, 25799, 1521, 537, 28151, 315, 279, 5480, 518, 678, 13, 220, 54006, 11, 279, 50564, 12729, 458, 14303, 13, 220, 1124, 77, 1699, 1925, 5534, 220, 16, 11, 220, 17, 15, 15, 22, 11, 279, 3639, 4180, 7154, 315, 47107, 369, 279, 96488, 27218, 10897, 264, 32169, 19407, 311, 279, 10942, 7154, 320, 60256, 59927, 568, 220, 576, 27218, 7154, 25104, 279, 10942, 7154, 311, 3395, 279, 14613, 315, 2441, 3213, 1142, 11, 758, 1032, 9145, 16462, 69017, 40852, 63713, 38251, 17930, 11, 220, 19, 22, 24, 434, 13, 18, 67, 220, 24, 18, 21, 320, 23, 339, 40909, 13, 220, 17, 15, 15, 22, 568, 220, 1096, 1142, 14766, 264, 17408, 429, 572, 42493, 518, 20785, 448, 279, 5480, 315, 279, 10942, 7154, 389, 5534, 220, 22, 11, 220, 17, 15, 15, 21, 7110, 77, 1699, 1925, 6527, 220, 17, 17, 11, 220, 17, 15, 15, 22, 11, 279, 10942, 7154, 320, 60256, 59927, 8, 9283, 657, 1181, 3681, 5480, 11, 323, 11457, 264, 5480, 304, 4694, 315, 279, 25799, 13, 220, 576, 1142, 572, 7877, 279, 1790, 1899, 389, 6527, 220, 17, 18, 11, 220, 17, 15, 15, 22, 7110, 77, 1699, 497, 330, 19237, 16673, 788, 330, 1986, 1142, 572, 7117, 304, 220, 17, 15, 15, 19, 553, 264, 8778, 4741, 7369, 30289, 9364, 2348, 7369, 30289, 21863, 13, 304, 279, 547, 808, 13, 10942, 7154, 369, 279, 10867, 10942, 315, 24378, 13, 220, 576, 42002, 10491, 429, 7369, 30289, 11, 11689, 279, 2813, 594, 2820, 8113, 4842, 11, 14238, 15479, 2348, 3198, 11, 323, 1340, 16105, 9445, 5269, 323, 5811, 19931, 533, 15957, 11, 438, 1632, 438, 25129, 13, 220, 576, 7154, 13214, 14820, 279, 42002, 594, 11379, 369, 536, 27606, 11, 714, 2937, 27437, 1181, 33913, 323, 11676, 12126, 19407, 311, 42002, 11, 2777, 7766, 264, 536, 311, 8253, 19857, 13, 220, 4354, 11, 279, 7154, 315, 47107, 13862, 279, 10942, 7154, 20207, 311, 264, 9760, 1142, 892, 17551, 264, 8645, 311, 264, 4428, 2025, 11, 27492, 24913, 279, 7154, 311, 9283, 349, 1181, 4867, 17408, 323, 4265, 19407, 304, 4694, 315, 39146, 389, 6527, 220, 17, 17, 11, 220, 17, 15, 15, 22, 1189, 532, 73594, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a professional NLP data parser.\n",
      "Follow the provided `Task` by the user to generate the `Output JSON`.\n",
      "Do not generate any introduction or conclusion.<|im_end|>\n",
      "<|im_start|>user\n",
      "# Document:\n",
      "Page 1\n",
      "LEXSEE 2003 U.S. DIST. CT. PLEADINGS 3030 View U.S. District Court Opinion View Original Source Image of This Document\n",
      "SUSAN STOCKING, for herself and all other similarly situated, Plaintiff, v. AT&T CORP., Defendant.\n",
      "Case No. 03-0421-CV-W-HFS UNITED STATES DISTRICT COURT FOR THE WESTERN DISTRICT OF\n",
      "MISSOURI, WESTERN DIVISION 2003 U.S. Dist. Ct. Pleadings 3030; 2004 U.S. Dist. Ct. Pleadings LEXIS 9181\n",
      "January 23, 2004 Complaint\n",
      "VIEW OTHER AVAILABLE CONTENT RELATED TO THIS DOCUMENT: U.S. District Court: Motion(s) COUNSEL: [**1] Sylvester \"Sly\" James, Jr. MO # 33617, Michael J. Mohlman KS # 19084, The Sly James Firm Trial Lawyers, P.C., 802 Broadway, 7th Floor, Kansas City, MO 64105, 816-472-6800, 816-472-6805 facsimile and Rex A. Sharp KS # 12350, Gunderson, Sharp & Rhein, P.C., 4121 W. 83rd St., Ste. 256, Prairie Village, KS 66208, 913-901-0500, 913-901-0419 facsimile and Rick D. Holtsclaw MO # 32866, Holtsclaw & Kendall, LC, 312 West 8th Street, Kansas City, MO 64105, 816-221-2555, 816-221-8763 facsimile. ATTORNEYS FOR PLAINTIFFS. TITLE: FIRST AMENDED CLASS ACTION COMPLAINT TEXT: [*1] Plaintiff Susan Stocking brings this case on behalf of herself and all other women employed by AT&T Corp. (\"AT&T\") who have been discriminated against by their employer, AT&T, and its affiliated companies, and would show the Court as follows: I. Jurisdiction, Venue, and Parties\n",
      "1. Plaintiff Susan Stocking is a citizen of the state of Missouri, was employed by Defendant, and has filed an EEOC complaint and obtained the right to sue. See attached Decision (attached as \"Exhibit A\") and right-to-sue letter (attached as \"Exhibit B\").\n",
      "2. AT&T employs Plaintiff and thousands [**2] of other women like her. AT&T is a citizen of the states of New\n",
      "\n",
      "\f2003 U.S. Dist. Ct. Pleadings 3030, *1; 2004 U.S. Dist. Ct. Pleadings LEXIS 9181, **2\n",
      "\n",
      "Page 2\n",
      "\n",
      "York where it is incorporated and of New Jersey where its principal place of business is located; and can be served with service of process at The Corporation Company, Inc., 515 S. Kansas Ave., Topeka, KS 66603.\n",
      "3. Aetna Insurance Company (\"Health Insurer\") was a health insurer for Plaintiff and/or AT&T.\n",
      "[*2] 4. Venue is proper in this Court as Defendant does business in this judicial district and has offered or provided health insurance to its employees in this judicial district. 28 U.S.C. § 1391 (b) and (c); 42 U.S.C. § 2000e 5(f)(3).\n",
      "5. This Court has subject matter jurisdiction based on federal question jurisdiction, 28 U.S.C. § 1331, and 42 U.S.C. § 2000e - 5(f)(3).\n",
      "A. Summary of Claims\n",
      "6. Prescription medication related to reproduction is routinely covered for men, but not for women. Prescription contraception, which is used only by women, is a basic prescription medication related to reproduction; specifically, it prevents pregnancy for women who wish to do so by reversible means. Plaintiff, on [**3] behalf of herself and all others similarly situated, brings this class action against AT&T for sex discrimination in violation of Title VII of the Civil Rights Act of 1964, 42 U.S.C. § 2000e et seq. and the Pregnancy Discrimination Act (PDA), 42 U.S.C. § 2000e(k) for offering and/or providing health insurance that does not cover prescription contraceptives for women related to reproduction while covering sex-related prescription medication for men, such as Viagra.\n",
      "7. AT&T's exclusion of prescription contraception has an adverse disparate impact on Ms. Stocking and other members of the proposed class. Because prescription contraceptives are available for use only by women, AT&T's failure to provide coverage for prescription contraception forces its female employees to choose between paying their own out-of-pocket prescription costs, or bearing the physical, emotional and financial costs of an unplanned pregnancy.\n",
      "8. As a result of AT&T decision to exclude contraceptives from its benefits plan, Ms. Stocking and other members of the proposed class are being discriminated against in the [*3] terms and conditions of employment, [**4] which includes receipt of benefits under fringe benefit programs, because of their potential for pregnancy. This violates Title VII.\n",
      "B. Plaintiff Susan Stocking\n",
      "9. Plaintiff Susan Stocking has been employed by AT&T on a full-time basis since June 12, 1995. As part of the terms and conditions of her employment, Ms. Stocking receives health insurance coverage, including coverage of prescription drugs and devices. Ms. Stocking sues on her own behalf and as a representative of the proposed class of employees who are discriminated against by the exclusion of contraception from AT&T's benefit plan.\n",
      "10. Ms. Stocking was, at all times relevant to this cause of action, a woman of childbearing age who was concerned with the prevention of an unwanted or unplanned pregnancy. Ms. Stocking requested that her prescription health care benefit provide or reimburse her for the cost of birth control. Her requests were denied. Subsequently, Ms. Stocking learned that men were provided and received plan coverage of a full range of conditions that can be treated through prescription medication.\n",
      "11. On August 26, 2002, Ms. Stocking filed a charge with the EEOC at its Kansas City Area Office [**5] in Kansas City, Kansas alleging that AT&T's failure to provide her with health insurance coverage for prescription contraceptives constitutes unlawful discrimination on the basis of sex.\n",
      "12. As a result, Ms. Stocking received a decision and a right-to-sue letter from the EEOC (copies of which are attached hereto as \"Exhibit A\" and Exhibit B\").\n",
      "\n",
      "\f2003 U.S. Dist. Ct. Pleadings 3030, *3; 2004 U.S. Dist. Ct. Pleadings LEXIS 9181, **5\n",
      "\n",
      "Page 3\n",
      "\n",
      "[*4] 13. On November 29, 2002, the EEOC issued a \"Determination\" to Plaintiff Susan Stocking, in which it considered AT&T's benefit plan and concluded that:\n",
      "\"...it is the Commission's position that the pre-July 2002 exclusion violates both Title VII and the PDA, since the statutes cover prescription contraceptives regardless of their intended purpose. Prescription contraceptives are available only for women. As a result, Respondent's pre-July 2002 policy is, by definition, a sex-based exclusion...It appears that Respondent's current plan uniquely classifies contraceptives used for birth control purposes by requiring their acquisition solely by mail. This also represents a sex-based distinction and violates the statutes. Coverage for prescription contraceptives must be afforded in the same manner as if the woman, or any employee, [**6] sought other preventive or health maintenance services.\n",
      "14. AT&T's refusal to provide the same benefits to both men and women has caused an economic hardship on Ms. Stocking and other members of the proposed class that male employees are not required to endure.\n",
      "II. AT&T's Health Plan\n",
      "15. As terms and conditions of their employment, AT&T offered Ms. Stocking the opportunity to enroll in one of three health plans (Plan). The three options were: Post of Service (POS) option; Traditional Indemnity option (PPO); and Health Maintenance Organization (HMO). The POS offered the highest level of benefits. The PPO option was only offered to those employees who lived outside the POS service area, but allowed selection of physicians and reimbursement after the deductible was met. The HMO option generally had no deductibles to meet, a small co-payment but required participants to see a doctor in the HMO network.\n",
      "16. Regular full-time occupational employees are eligible for coverage at the company's expense on the first day of the month in which the employee attains six months of Net Certified Service with AT&T. Regular full-time employees are not required to contribute to receive [**7] Plan benefits, but with the HMO option, they may be required to do so.\n",
      "[*5] 17. Ms. Stocking elected to take advantage of the PPO option.\n",
      "18. The Plan also provides for a Prescription Drug Benefit Plan which was administered by Merck-Medco (Drug Plan). The Drug Plan provided drug benefits to participants in the POS and PPO options. In general, the Drug Plan covered all drugs provided by a pharmacist at the physician's order unless specifically excluded. The Drug Plan specifically excluded \"[a]ny drugs or medications used solely for birth control, including oral contraceptives, jellies, foams, devices, implants or injections.\"\n",
      "19. Despite covering other preventative medical services and prescriptions, neither the Plan nor the Drug Plan provided for prescription drugs and/or devices used by plaintiffs and other women similarly situated to prevent pregnancy.\n",
      "20. If plaintiff became pregnant, however, the Plan would have covered the costs of either an abortion or continuing the pregnancy to term - whichever she chose to do.\n",
      "III. Harm to Ms. Stocking and Other Class Members\n",
      "21. As a direct and proximate result of the AT&T Plan's failure to cover contraception [**8] to prevent pregnancy, Ms. Stocking and other proposed members of the class were required to pay for their monthly or periodic contraceptives on an out-of-pocket basis, or risk unintended pregnancy. Like many healthy women of reproductive age, contraception is the only prescription drug Ms. Stocking used on a regular and consistent basis.\n",
      "22. On information and belief, AT&T employs hundreds of women of reproductive age who use prescription contraception.\n",
      "\n",
      "\f2003 U.S. Dist. Ct. Pleadings 3030, *5; 2004 U.S. Dist. Ct. Pleadings LEXIS 9181, **8\n",
      "\n",
      "Page 4\n",
      "\n",
      "23. If contraception were treated on an equal basis with other prescriptions under the Plan and the Drug Plan, Ms. Stocking and other proposed class members would have paid a low co-pay, or no co-pay at all, for generic or brand-name, 34 day supply, of contraception.\n",
      "[*6] IV. Factual Framework\n",
      "24. For over thirty years of their lives, women have the biological potential for pregnancy```json\n",
      "{\"Summary_long\": \"On January 23, 2004, Plaintiff filed an amended complaint under Title VII of the Civil Rights Act of 1964 and the Pregnancy Discrimination Act, 42 U.S.C. \\u00a7 2000e et seq. and 2000e(k), against AT&T Corporation in the United States District Court for the Western District of Missouri.  The plaintiff, represented by private counsel, was a former AT&T employee and asked the Court for declaratory and injunctive relief, as well as damages, alleging that AT&T's health insurance policy discriminated against women.  Specifically, the plaintiff contended that the defendant's health insurance plan, which did not provide prescription contraceptives (birth control) before 2002 and only through the mail after 2002, violated female employees' civil rights.\\n\\nThe action originally started in the U.S. District Court for the District of Kansas, but was transferred to Missouri on May 12, 2003.  This is the date on which the docket begins.  Originally, there were two plaintiffs on the case, but one dropped out of the litigation.  Thus, a single plaintiff carried forth the case.\\n\\nOn September 3, 2004, the Court (Judge Sachs) denied the plaintiff's motion for class certification.  In the opinion, the Court barely discussed the requirements for a class-action lawsuit, except to point out typicality.  The plaintiff, since the suit began, had stopped taking birth control and was seeking to get pregnant.  Therefore, the Court declared that a class could not be certified without a class representative who would have an ongoing interest in the prayed-for relief.  The plaintiff wanted to recover her past expenditures on the prescribed contraceptives.  The Women's Law project came onto the case as an amicus.  After a lengthy discovery period, the Court made another ruling.\\n\\nOn June 7, 2006, the Court (Judge Sachs) granted the plaintiff's motion for class certification as to the damages in the complaint.  The class was comprised of female employees of AT&T, who paid for their own prescription contraception from the dates of October 31, 2001 to July 2, 2002.  The Court did not certify the claims for injunctive or declaratory relief because as of 2002, the health insurance provider covered birth control through mail order. While the cost of the plaintiff's potential recovery was relatively small, $68.07, the class action potential in the litigation, given AT&T's size made both parties fight vigorously.  According to the docket, neither side was content with this decision.  The plaintiff wanted the period of time for damages claims to be extended, and the defendant did not approve of the decision at all.  Ultimately, the plaintiffs filed an appeal.  \\n\\nOn June 1, 2007, the United States Court of Appeals for the Eighth Circuit issued a preliminary judgment to the District Court (Judge Sachs).  The Circuit Court advised the District Court to review the findings of another recent case, In Re Union Pacific Railroad Employment Practices Litigation, 479 F.3d 936 (8th Cir. 2007).  This case featured a ruling that was fundamentally at odds with the decision of the District Court on June 7, 2006.\\n\\nOn October 22, 2007, the District Court (Judge Sachs) vacated its previous decision, and ordered a decision in favor of the defendant.  The case was closed the next day on October 23, 2007.\\n\\n\", \"Summary_short\": \"This case was brought in 2004 by a female former AT&T employee against AT&T Corp. in the U.S. District Court for the Western District of Missouri.  The plaintiff alleged that AT&T, specifically the company's health insurance policy, discriminated against women, and she sought declaratory and injunctive relief, as well as damages.  The Court originally denied the plaintiff's motion for class certification, but later reversed its denial and granted summary judgment to plaintiff, certifying a class to determine compensation.  However, the Court of Appeals referred the District Court Judge to a relevant case which rejected a challenge to a similar program, thereby forcing the Court to vacate its prior ruling and issue judgment in favor of defendants on October 22, 2007.\"}\n",
      "```<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 19237, 17799, 788, 330, 1925, 6058, 220, 17, 18, 11, 220, 17, 15, 15, 19, 11, 77282, 12729, 458, 29911, 12181, 1212, 10869, 44969, 315, 279, 16398, 10512, 3218, 315, 220, 16, 24, 21, 19, 323, 279, 82527, 77300, 2554, 3218, 11, 220, 19, 17, 547, 808, 727, 13, 1124, 84, 15, 15, 64, 22, 220, 17, 15, 15, 15, 68, 1842, 12981, 13, 323, 220, 17, 15, 15, 15, 68, 5969, 701, 2348, 7369, 30289, 13034, 304, 279, 3639, 4180, 10942, 7154, 369, 279, 10867, 10942, 315, 24378, 13, 220, 576, 42002, 11, 15251, 553, 869, 16080, 11, 572, 264, 4741, 7369, 30289, 9364, 323, 4588, 279, 7154, 369, 9445, 5269, 323, 5811, 19931, 533, 15957, 11, 438, 1632, 438, 25129, 11, 61446, 429, 7369, 30289, 594, 2820, 8113, 4842, 14238, 15479, 2348, 3198, 13, 220, 44763, 11, 279, 42002, 683, 2883, 429, 279, 25799, 594, 2820, 8113, 3119, 11, 892, 1521, 537, 3410, 22091, 44836, 417, 1886, 320, 27713, 2524, 8, 1573, 220, 17, 15, 15, 17, 323, 1172, 1526, 279, 8072, 1283, 220, 17, 15, 15, 17, 11, 33421, 8778, 8256, 6, 8267, 3188, 7110, 77, 1699, 785, 1917, 13214, 3855, 304, 279, 547, 808, 13, 10942, 7154, 369, 279, 10942, 315, 20148, 11, 714, 572, 22409, 311, 24378, 389, 3217, 220, 16, 17, 11, 220, 17, 15, 15, 18, 13, 220, 1096, 374, 279, 2400, 389, 892, 279, 653, 3996, 12033, 13, 220, 24805, 11, 1052, 1033, 1378, 50564, 389, 279, 1142, 11, 714, 825, 12226, 700, 315, 279, 38625, 13, 220, 14301, 11, 264, 3175, 42002, 11691, 13241, 279, 1142, 7110, 77, 1699, 1925, 6122, 220, 18, 11, 220, 17, 15, 15, 19, 11, 279, 7154, 320, 60256, 59927, 8, 14820, 279, 42002, 594, 11379, 369, 536, 27606, 13, 220, 758, 279, 9459, 11, 279, 7154, 19460, 14078, 279, 8502, 369, 264, 536, 25059, 19275, 11, 3650, 311, 1459, 700, 14260, 487, 13, 220, 576, 42002, 11, 2474, 279, 7781, 6009, 11, 1030, 10497, 4633, 7194, 2524, 323, 572, 10887, 311, 633, 20280, 13, 220, 15277, 11, 279, 7154, 14275, 429, 264, 536, 1410, 537, 387, 22909, 2041, 264, 536, 18239, 879, 1035, 614, 458, 14195, 2734, 304, 279, 66697, 15193, 15957, 13, 220, 576, 42002, 4829, 311, 11731, 1059, 3267, 52769, 389, 279, 30931, 44836, 417, 1886, 13, 220, 576, 10973, 594, 7506, 2390, 3697, 8630, 279, 1142, 438, 458, 1079, 58244, 13, 220, 4636, 264, 34206, 18335, 4168, 11, 279, 7154, 1865, 2441, 17408, 7110, 77, 1699, 1925, 5534, 220, 22, 11, 220, 17, 15, 15, 21, 11, 279, 7154, 320, 60256, 59927, 8, 11676, 279, 42002, 594, 11379, 369, 536, 27606, 438, 311, 279, 25129, 304, 279, 12181, 13, 220, 576, 536, 572, 38956, 315, 8778, 8256, 315, 7369, 30289, 11, 879, 7171, 369, 862, 1828, 22091, 71254, 504, 279, 12713, 315, 6527, 220, 18, 16, 11, 220, 17, 15, 15, 16, 311, 5768, 220, 17, 11, 220, 17, 15, 15, 17, 13, 220, 576, 7154, 1521, 537, 92230, 279, 8186, 369, 5811, 19931, 533, 476, 9445, 5269, 15957, 1576, 438, 315, 220, 17, 15, 15, 17, 11, 279, 2820, 8113, 9109, 9761, 7194, 2524, 1526, 8072, 1973, 13, 5976, 279, 2783, 315, 279, 42002, 594, 4650, 13351, 572, 12040, 2613, 11, 400, 21, 23, 13, 15, 22, 11, 279, 536, 1917, 4650, 304, 279, 38625, 11, 2661, 7369, 30289, 594, 1379, 1865, 2176, 9677, 4367, 76749, 13, 220, 10548, 311, 279, 653, 3996, 11, 13866, 3108, 572, 2213, 448, 419, 5480, 13, 220, 576, 42002, 4829, 279, 4168, 315, 882, 369, 25129, 8186, 311, 387, 11577, 11, 323, 279, 25799, 1521, 537, 28151, 315, 279, 5480, 518, 678, 13, 220, 54006, 11, 279, 50564, 12729, 458, 14303, 13, 220, 1124, 77, 1699, 1925, 5534, 220, 16, 11, 220, 17, 15, 15, 22, 11, 279, 3639, 4180, 7154, 315, 47107, 369, 279, 96488, 27218, 10897, 264, 32169, 19407, 311, 279, 10942, 7154, 320, 60256, 59927, 568, 220, 576, 27218, 7154, 25104, 279, 10942, 7154, 311, 3395, 279, 14613, 315, 2441, 3213, 1142, 11, 758, 1032, 9145, 16462, 69017, 40852, 63713, 38251, 17930, 11, 220, 19, 22, 24, 434, 13, 18, 67, 220, 24, 18, 21, 320, 23, 339, 40909, 13, 220, 17, 15, 15, 22, 568, 220, 1096, 1142, 14766, 264, 17408, 429, 572, 42493, 518, 20785, 448, 279, 5480, 315, 279, 10942, 7154, 389, 5534, 220, 22, 11, 220, 17, 15, 15, 21, 7110, 77, 1699, 1925, 6527, 220, 17, 17, 11, 220, 17, 15, 15, 22, 11, 279, 10942, 7154, 320, 60256, 59927, 8, 9283, 657, 1181, 3681, 5480, 11, 323, 11457, 264, 5480, 304, 4694, 315, 279, 25799, 13, 220, 576, 1142, 572, 7877, 279, 1790, 1899, 389, 6527, 220, 17, 18, 11, 220, 17, 15, 15, 22, 7110, 77, 1699, 497, 330, 19237, 16673, 788, 330, 1986, 1142, 572, 7117, 304, 220, 17, 15, 15, 19, 553, 264, 8778, 4741, 7369, 30289, 9364, 2348, 7369, 30289, 21863, 13, 304, 279, 547, 808, 13, 10942, 7154, 369, 279, 10867, 10942, 315, 24378, 13, 220, 576, 42002, 10491, 429, 7369, 30289, 11, 11689, 279, 2813, 594, 2820, 8113, 4842, 11, 14238, 15479, 2348, 3198, 11, 323, 1340, 16105, 9445, 5269, 323, 5811, 19931, 533, 15957, 11, 438, 1632, 438, 25129, 13, 220, 576, 7154, 13214, 14820, 279, 42002, 594, 11379, 369, 536, 27606, 11, 714, 2937, 27437, 1181, 33913, 323, 11676, 12126, 19407, 311, 42002, 11, 2777, 7766, 264, 536, 311, 8253, 19857, 13, 220, 4354, 11, 279, 7154, 315, 47107, 13862, 279, 10942, 7154, 20207, 311, 264, 9760, 1142, 892, 17551, 264, 8645, 311, 264, 4428, 2025, 11, 27492, 24913, 279, 7154, 311, 9283, 349, 1181, 4867, 17408, 323, 4265, 19407, 304, 4694, 315, 39146, 389, 6527, 220, 17, 17, 11, 220, 17, 15, 15, 22, 1189, 532, 73594, 151645, 198]\n",
      "labels:\n",
      "```json\n",
      "{\"Summary_long\": \"On January 23, 2004, Plaintiff filed an amended complaint under Title VII of the Civil Rights Act of 1964 and the Pregnancy Discrimination Act, 42 U.S.C. \\u00a7 2000e et seq. and 2000e(k), against AT&T Corporation in the United States District Court for the Western District of Missouri.  The plaintiff, represented by private counsel, was a former AT&T employee and asked the Court for declaratory and injunctive relief, as well as damages, alleging that AT&T's health insurance policy discriminated against women.  Specifically, the plaintiff contended that the defendant's health insurance plan, which did not provide prescription contraceptives (birth control) before 2002 and only through the mail after 2002, violated female employees' civil rights.\\n\\nThe action originally started in the U.S. District Court for the District of Kansas, but was transferred to Missouri on May 12, 2003.  This is the date on which the docket begins.  Originally, there were two plaintiffs on the case, but one dropped out of the litigation.  Thus, a single plaintiff carried forth the case.\\n\\nOn September 3, 2004, the Court (Judge Sachs) denied the plaintiff's motion for class certification.  In the opinion, the Court barely discussed the requirements for a class-action lawsuit, except to point out typicality.  The plaintiff, since the suit began, had stopped taking birth control and was seeking to get pregnant.  Therefore, the Court declared that a class could not be certified without a class representative who would have an ongoing interest in the prayed-for relief.  The plaintiff wanted to recover her past expenditures on the prescribed contraceptives.  The Women's Law project came onto the case as an amicus.  After a lengthy discovery period, the Court made another ruling.\\n\\nOn June 7, 2006, the Court (Judge Sachs) granted the plaintiff's motion for class certification as to the damages in the complaint.  The class was comprised of female employees of AT&T, who paid for their own prescription contraception from the dates of October 31, 2001 to July 2, 2002.  The Court did not certify the claims for injunctive or declaratory relief because as of 2002, the health insurance provider covered birth control through mail order. While the cost of the plaintiff's potential recovery was relatively small, $68.07, the class action potential in the litigation, given AT&T's size made both parties fight vigorously.  According to the docket, neither side was content with this decision.  The plaintiff wanted the period of time for damages claims to be extended, and the defendant did not approve of the decision at all.  Ultimately, the plaintiffs filed an appeal.  \\n\\nOn June 1, 2007, the United States Court of Appeals for the Eighth Circuit issued a preliminary judgment to the District Court (Judge Sachs).  The Circuit Court advised the District Court to review the findings of another recent case, In Re Union Pacific Railroad Employment Practices Litigation, 479 F.3d 936 (8th Cir. 2007).  This case featured a ruling that was fundamentally at odds with the decision of the District Court on June 7, 2006.\\n\\nOn October 22, 2007, the District Court (Judge Sachs) vacated its previous decision, and ordered a decision in favor of the defendant.  The case was closed the next day on October 23, 2007.\\n\\n\", \"Summary_short\": \"This case was brought in 2004 by a female former AT&T employee against AT&T Corp. in the U.S. District Court for the Western District of Missouri.  The plaintiff alleged that AT&T, specifically the company's health insurance policy, discriminated against women, and she sought declaratory and injunctive relief, as well as damages.  The Court originally denied the plaintiff's motion for class certification, but later reversed its denial and granted summary judgment to plaintiff, certifying a class to determine compensation.  However, the Court of Appeals referred the District Court Judge to a relevant case which rejected a challenge to a similar program, thereby forcing the Court to vacate its prior ruling and issue judgment in favor of defendants on October 22, 2007.\"}\n",
      "```<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 14:16:08,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 14:16:08,496 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-05-11 14:16:08] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "model.safetensors: 100%|████████████████████| 3.09G/3.09G [00:11<00:00, 261MB/s]\n",
      "[INFO|modeling_utils.py:3904] 2025-05-11 14:16:20,726 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n",
      "[INFO|modeling_utils.py:1582] 2025-05-11 14:16:20,756 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-05-11 14:16:20,759 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4888] 2025-05-11 14:16:22,280 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-05-11 14:16:22,280 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 2.18MB/s]\n",
      "[INFO|configuration_utils.py:1095] 2025-05-11 14:16:22,638 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-05-11 14:16:22,639 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-05-11 14:16:22] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-11 14:16:22] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-11 14:16:22] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-11 14:16:22] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-11 14:16:22] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,up_proj,k_proj,down_proj,v_proj,q_proj,o_proj\n",
      "[INFO|2025-05-11 14:16:24] llamafactory.model.loader:143 >> trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n",
      "[INFO|trainer.py:741] 2025-05-11 14:16:24,178 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2369] 2025-05-11 14:16:24,787 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-05-11 14:16:24,787 >>   Num examples = 3,177\n",
      "[INFO|trainer.py:2371] 2025-05-11 14:16:24,788 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2372] 2025-05-11 14:16:24,788 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2375] 2025-05-11 14:16:24,788 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2376] 2025-05-11 14:16:24,788 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2377] 2025-05-11 14:16:24,788 >>   Total optimization steps = 396\n",
      "[INFO|trainer.py:2378] 2025-05-11 14:16:24,792 >>   Number of trainable parameters = 73,859,072\n",
      "[INFO|integration_utils.py:817] 2025-05-11 14:16:25,242 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmedoahmed0120\u001b[0m (\u001b[33mmedoahmed0120-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LLaMA-Factory/wandb/run-20250511_141625-62puakb8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlegalx-finetune-llamafactory\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/medoahmed0120-student/llamafactory\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/medoahmed0120-student/llamafactory/runs/62puakb8\u001b[0m\n",
      "{'loss': 1.6354, 'grad_norm': 0.4938645660877228, 'learning_rate': 2.5e-05, 'epoch': 0.05}\n",
      "{'loss': 1.4946, 'grad_norm': 0.39254897832870483, 'learning_rate': 5e-05, 'epoch': 0.1}\n",
      "{'loss': 1.4617, 'grad_norm': 0.3532043397426605, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4042, 'grad_norm': 0.3172547519207001, 'learning_rate': 0.0001, 'epoch': 0.2}\n",
      "{'loss': 1.3916, 'grad_norm': 0.3404569625854492, 'learning_rate': 9.980543805476446e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3292, 'grad_norm': 0.37407952547073364, 'learning_rate': 9.922326639307917e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3464, 'grad_norm': 0.3272251486778259, 'learning_rate': 9.825801575298248e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3194, 'grad_norm': 0.3492641746997833, 'learning_rate': 9.691719817616147e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3167, 'grad_norm': 0.2957662045955658, 'learning_rate': 9.521124854565425e-05, 'epoch': 0.45}\n",
      "{'loss': 1.3419, 'grad_norm': 0.3375783860683441, 'learning_rate': 9.315344337660421e-05, 'epoch': 0.5}\n",
      " 25%|██████████▎                              | 100/396 [13:45<40:43,  8.25s/it][INFO|trainer.py:4226] 2025-05-11 14:30:12,359 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-05-11 14:30:12,359 >>   Num examples = 454\n",
      "[INFO|trainer.py:4231] 2025-05-11 14:30:12,360 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:31,  3.50it/s]\u001b[A\n",
      "  3%|█▏                                         | 3/114 [00:01<00:44,  2.47it/s]\u001b[A\n",
      "  4%|█▌                                         | 4/114 [00:01<00:51,  2.14it/s]\u001b[A\n",
      "  4%|█▉                                         | 5/114 [00:02<00:54,  1.99it/s]\u001b[A\n",
      "  5%|██▎                                        | 6/114 [00:02<00:56,  1.90it/s]\u001b[A\n",
      "  6%|██▋                                        | 7/114 [00:03<00:57,  1.85it/s]\u001b[A\n",
      "  7%|███                                        | 8/114 [00:04<00:58,  1.81it/s]\u001b[A\n",
      "  8%|███▍                                       | 9/114 [00:04<00:58,  1.79it/s]\u001b[A\n",
      "  9%|███▋                                      | 10/114 [00:05<00:58,  1.78it/s]\u001b[A\n",
      " 10%|████                                      | 11/114 [00:05<00:58,  1.77it/s]\u001b[A\n",
      " 11%|████▍                                     | 12/114 [00:06<00:57,  1.77it/s]\u001b[A\n",
      " 11%|████▊                                     | 13/114 [00:06<00:57,  1.76it/s]\u001b[A\n",
      " 12%|█████▏                                    | 14/114 [00:07<00:56,  1.76it/s]\u001b[A\n",
      " 13%|█████▌                                    | 15/114 [00:08<00:56,  1.75it/s]\u001b[A\n",
      " 14%|█████▉                                    | 16/114 [00:08<00:55,  1.75it/s]\u001b[A\n",
      " 15%|██████▎                                   | 17/114 [00:09<00:55,  1.75it/s]\u001b[A\n",
      " 16%|██████▋                                   | 18/114 [00:09<00:54,  1.75it/s]\u001b[A\n",
      " 17%|███████                                   | 19/114 [00:10<00:54,  1.75it/s]\u001b[A\n",
      " 18%|███████▎                                  | 20/114 [00:10<00:53,  1.75it/s]\u001b[A\n",
      " 18%|███████▋                                  | 21/114 [00:11<00:53,  1.75it/s]\u001b[A\n",
      " 19%|████████                                  | 22/114 [00:12<00:52,  1.75it/s]\u001b[A\n",
      " 20%|████████▍                                 | 23/114 [00:12<00:52,  1.75it/s]\u001b[A\n",
      " 21%|████████▊                                 | 24/114 [00:13<00:51,  1.75it/s]\u001b[A\n",
      " 22%|█████████▏                                | 25/114 [00:13<00:50,  1.75it/s]\u001b[A\n",
      " 23%|█████████▌                                | 26/114 [00:14<00:50,  1.75it/s]\u001b[A\n",
      " 24%|█████████▉                                | 27/114 [00:14<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▎                               | 28/114 [00:15<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▋                               | 29/114 [00:16<00:48,  1.75it/s]\u001b[A\n",
      " 26%|███████████                               | 30/114 [00:16<00:48,  1.75it/s]\u001b[A\n",
      " 27%|███████████▍                              | 31/114 [00:17<00:47,  1.75it/s]\u001b[A\n",
      " 28%|███████████▊                              | 32/114 [00:17<00:46,  1.75it/s]\u001b[A\n",
      " 29%|████████████▏                             | 33/114 [00:18<00:46,  1.75it/s]\u001b[A\n",
      " 30%|████████████▌                             | 34/114 [00:18<00:45,  1.75it/s]\u001b[A\n",
      " 31%|████████████▉                             | 35/114 [00:19<00:45,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 36/114 [00:20<00:44,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▋                            | 37/114 [00:20<00:44,  1.75it/s]\u001b[A\n",
      " 33%|██████████████                            | 38/114 [00:21<00:43,  1.75it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 39/114 [00:21<00:42,  1.75it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 40/114 [00:22<00:42,  1.75it/s]\u001b[A\n",
      " 36%|███████████████                           | 41/114 [00:22<00:41,  1.75it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 42/114 [00:23<00:41,  1.75it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 43/114 [00:24<00:40,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 44/114 [00:24<00:39,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▌                         | 45/114 [00:25<00:39,  1.75it/s]\u001b[A\n",
      " 40%|████████████████▉                         | 46/114 [00:25<00:38,  1.75it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:26<00:38,  1.75it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:26<00:37,  1.75it/s]\u001b[A\n",
      " 43%|██████████████████                        | 49/114 [00:27<00:37,  1.75it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:28<00:36,  1.75it/s]\u001b[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:28<00:35,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:29<00:35,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:29<00:34,  1.75it/s]\u001b[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:30<00:34,  1.75it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:30<00:33,  1.75it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:31<00:33,  1.75it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 57/114 [00:32<00:32,  1.75it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:32<00:31,  1.75it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:33<00:31,  1.75it/s]\u001b[A\n",
      " 53%|██████████████████████                    | 60/114 [00:33<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:34<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:34<00:29,  1.75it/s]\u001b[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:35<00:29,  1.75it/s]\u001b[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:36<00:28,  1.75it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:36<00:27,  1.75it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:37<00:27,  1.75it/s]\u001b[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:37<00:26,  1.75it/s]\u001b[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:38<00:26,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:38<00:25,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:39<00:25,  1.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:40<00:24,  1.75it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:40<00:23,  1.75it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:41<00:23,  1.75it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:41<00:22,  1.75it/s]\u001b[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:42<00:22,  1.75it/s]\u001b[A\n",
      " 67%|████████████████████████████              | 76/114 [00:42<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:43<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:44<00:20,  1.75it/s]\u001b[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:44<00:19,  1.75it/s]\u001b[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:45<00:19,  1.75it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:45<00:18,  1.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [00:46<00:18,  1.75it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [00:46<00:17,  1.75it/s]\u001b[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [00:47<00:17,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [00:48<00:16,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [00:48<00:15,  1.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████          | 87/114 [00:49<00:15,  1.75it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [00:49<00:14,  1.75it/s]\u001b[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [00:50<00:14,  1.75it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [00:50<00:13,  1.75it/s]\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [00:51<00:13,  1.75it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [00:51<00:12,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [00:52<00:11,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [00:53<00:11,  1.75it/s]\u001b[A\n",
      " 83%|███████████████████████████████████       | 95/114 [00:53<00:10,  1.75it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [00:54<00:10,  1.75it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [00:54<00:09,  1.75it/s]\u001b[A\n",
      " 86%|████████████████████████████████████      | 98/114 [00:55<00:09,  1.75it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [00:55<00:08,  1.75it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [00:56<00:07,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [00:57<00:07,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [00:57<00:06,  1.75it/s]\u001b[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [00:58<00:06,  1.75it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [00:58<00:05,  1.75it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [00:59<00:05,  1.75it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [00:59<00:04,  1.75it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:00<00:03,  1.75it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:01<00:03,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:01<00:02,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:02<00:02,  1.75it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:02<00:01,  1.75it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:03<00:01,  1.75it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:03<00:00,  1.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.256766438484192, 'eval_runtime': 65.1421, 'eval_samples_per_second': 6.969, 'eval_steps_per_second': 1.75, 'epoch': 0.5}\n",
      " 25%|██████████▎                              | 100/396 [14:50<40:43,  8.25s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:04<00:00,  1.76it/s]\u001b[A\n",
      "{'loss': 1.3132, 'grad_norm': 0.33491113781929016, 'learning_rate': 9.075979749207561e-05, 'epoch': 0.55}\n",
      "{'loss': 1.282, 'grad_norm': 0.28892555832862854, 'learning_rate': 8.80489393880484e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3125, 'grad_norm': 0.3320489525794983, 'learning_rate': 8.504196625756166e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2663, 'grad_norm': 0.2532772123813629, 'learning_rate': 8.176227980227694e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2865, 'grad_norm': 0.2738679051399231, 'learning_rate': 7.823540410925435e-05, 'epoch': 0.75}\n",
      " 38%|███████████████▌                         | 150/396 [21:43<33:51,  8.26s/it][INFO|trainer.py:3910] 2025-05-11 14:38:10,401 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/checkpoint-150\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 14:38:10,560 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 14:38:10,561 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 14:38:11,137 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/checkpoint-150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 14:38:11,137 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/checkpoint-150/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 14:38:12,224 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 14:38:12,224 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "{'loss': 1.3053, 'grad_norm': 0.30484291911125183, 'learning_rate': 7.448878701031142e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2887, 'grad_norm': 0.36748555302619934, 'learning_rate': 7.055158646988109e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2907, 'grad_norm': 0.2903313934803009, 'learning_rate': 6.64544436638005e-05, 'epoch': 0.91}\n",
      "{'loss': 1.2929, 'grad_norm': 0.28638043999671936, 'learning_rate': 6.222924451504001e-05, 'epoch': 0.96}\n",
      "{'loss': 1.4094, 'grad_norm': 0.3236408829689026, 'learning_rate': 5.79088715422152e-05, 'epoch': 1.01}\n",
      " 51%|████████████████████▋                    | 200/396 [28:40<28:21,  8.68s/it][INFO|trainer.py:4226] 2025-05-11 14:45:07,323 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-05-11 14:45:07,324 >>   Num examples = 454\n",
      "[INFO|trainer.py:4231] 2025-05-11 14:45:07,324 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:32,  3.49it/s]\u001b[A\n",
      "  3%|█▏                                         | 3/114 [00:01<00:44,  2.48it/s]\u001b[A\n",
      "  4%|█▌                                         | 4/114 [00:01<00:51,  2.14it/s]\u001b[A\n",
      "  4%|█▉                                         | 5/114 [00:02<00:54,  1.99it/s]\u001b[A\n",
      "  5%|██▎                                        | 6/114 [00:02<00:56,  1.90it/s]\u001b[A\n",
      "  6%|██▋                                        | 7/114 [00:03<00:57,  1.85it/s]\u001b[A\n",
      "  7%|███                                        | 8/114 [00:03<00:58,  1.82it/s]\u001b[A\n",
      "  8%|███▍                                       | 9/114 [00:04<00:58,  1.79it/s]\u001b[A\n",
      "  9%|███▋                                      | 10/114 [00:05<00:58,  1.78it/s]\u001b[A\n",
      " 10%|████                                      | 11/114 [00:05<00:58,  1.77it/s]\u001b[A\n",
      " 11%|████▍                                     | 12/114 [00:06<00:57,  1.76it/s]\u001b[A\n",
      " 11%|████▊                                     | 13/114 [00:06<00:57,  1.76it/s]\u001b[A\n",
      " 12%|█████▏                                    | 14/114 [00:07<00:56,  1.76it/s]\u001b[A\n",
      " 13%|█████▌                                    | 15/114 [00:08<00:56,  1.76it/s]\u001b[A\n",
      " 14%|█████▉                                    | 16/114 [00:08<00:55,  1.75it/s]\u001b[A\n",
      " 15%|██████▎                                   | 17/114 [00:09<00:55,  1.75it/s]\u001b[A\n",
      " 16%|██████▋                                   | 18/114 [00:09<00:54,  1.75it/s]\u001b[A\n",
      " 17%|███████                                   | 19/114 [00:10<00:54,  1.75it/s]\u001b[A\n",
      " 18%|███████▎                                  | 20/114 [00:10<00:53,  1.75it/s]\u001b[A\n",
      " 18%|███████▋                                  | 21/114 [00:11<00:53,  1.75it/s]\u001b[A\n",
      " 19%|████████                                  | 22/114 [00:12<00:52,  1.75it/s]\u001b[A\n",
      " 20%|████████▍                                 | 23/114 [00:12<00:51,  1.75it/s]\u001b[A\n",
      " 21%|████████▊                                 | 24/114 [00:13<00:51,  1.75it/s]\u001b[A\n",
      " 22%|█████████▏                                | 25/114 [00:13<00:50,  1.75it/s]\u001b[A\n",
      " 23%|█████████▌                                | 26/114 [00:14<00:50,  1.75it/s]\u001b[A\n",
      " 24%|█████████▉                                | 27/114 [00:14<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▎                               | 28/114 [00:15<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▋                               | 29/114 [00:16<00:48,  1.75it/s]\u001b[A\n",
      " 26%|███████████                               | 30/114 [00:16<00:47,  1.75it/s]\u001b[A\n",
      " 27%|███████████▍                              | 31/114 [00:17<00:47,  1.75it/s]\u001b[A\n",
      " 28%|███████████▊                              | 32/114 [00:17<00:46,  1.75it/s]\u001b[A\n",
      " 29%|████████████▏                             | 33/114 [00:18<00:46,  1.75it/s]\u001b[A\n",
      " 30%|████████████▌                             | 34/114 [00:18<00:45,  1.75it/s]\u001b[A\n",
      " 31%|████████████▉                             | 35/114 [00:19<00:45,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 36/114 [00:20<00:44,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▋                            | 37/114 [00:20<00:44,  1.75it/s]\u001b[A\n",
      " 33%|██████████████                            | 38/114 [00:21<00:43,  1.75it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 39/114 [00:21<00:42,  1.75it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 40/114 [00:22<00:42,  1.75it/s]\u001b[A\n",
      " 36%|███████████████                           | 41/114 [00:22<00:41,  1.75it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 42/114 [00:23<00:41,  1.75it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 43/114 [00:24<00:40,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 44/114 [00:24<00:40,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▌                         | 45/114 [00:25<00:39,  1.75it/s]\u001b[A\n",
      " 40%|████████████████▉                         | 46/114 [00:25<00:38,  1.75it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:26<00:38,  1.75it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:26<00:37,  1.75it/s]\u001b[A\n",
      " 43%|██████████████████                        | 49/114 [00:27<00:37,  1.75it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:28<00:36,  1.75it/s]\u001b[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:28<00:36,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:29<00:35,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:29<00:34,  1.75it/s]\u001b[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:30<00:34,  1.75it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:30<00:33,  1.75it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:31<00:33,  1.75it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 57/114 [00:32<00:32,  1.75it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:32<00:32,  1.75it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:33<00:31,  1.75it/s]\u001b[A\n",
      " 53%|██████████████████████                    | 60/114 [00:33<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:34<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:34<00:29,  1.75it/s]\u001b[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:35<00:29,  1.75it/s]\u001b[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:36<00:28,  1.75it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:36<00:27,  1.75it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:37<00:27,  1.75it/s]\u001b[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:37<00:26,  1.75it/s]\u001b[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:38<00:26,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:38<00:25,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:39<00:25,  1.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:40<00:24,  1.75it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:40<00:23,  1.75it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:41<00:23,  1.75it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:41<00:22,  1.75it/s]\u001b[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:42<00:22,  1.75it/s]\u001b[A\n",
      " 67%|████████████████████████████              | 76/114 [00:42<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:43<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:44<00:20,  1.75it/s]\u001b[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:44<00:19,  1.75it/s]\u001b[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:45<00:19,  1.75it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:45<00:18,  1.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [00:46<00:18,  1.75it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [00:46<00:17,  1.75it/s]\u001b[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [00:47<00:17,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [00:48<00:16,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [00:48<00:15,  1.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████          | 87/114 [00:49<00:15,  1.75it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [00:49<00:14,  1.75it/s]\u001b[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [00:50<00:14,  1.75it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [00:50<00:13,  1.75it/s]\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [00:51<00:13,  1.75it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [00:52<00:12,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [00:52<00:11,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [00:53<00:11,  1.75it/s]\u001b[A\n",
      " 83%|███████████████████████████████████       | 95/114 [00:53<00:10,  1.75it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [00:54<00:10,  1.75it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [00:54<00:09,  1.75it/s]\u001b[A\n",
      " 86%|████████████████████████████████████      | 98/114 [00:55<00:09,  1.75it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [00:56<00:08,  1.75it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [00:56<00:07,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [00:57<00:07,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [00:57<00:06,  1.75it/s]\u001b[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [00:58<00:06,  1.75it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [00:58<00:05,  1.75it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [00:59<00:05,  1.75it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [01:00<00:04,  1.75it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:00<00:03,  1.75it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:01<00:03,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:01<00:02,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:02<00:02,  1.75it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:02<00:01,  1.75it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:03<00:01,  1.75it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:04<00:00,  1.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2174899578094482, 'eval_runtime': 65.1629, 'eval_samples_per_second': 6.967, 'eval_steps_per_second': 1.749, 'epoch': 1.01}\n",
      " 51%|████████████████████▋                    | 200/396 [29:45<28:21,  8.68s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:04<00:00,  1.76it/s]\u001b[A\n",
      "{'loss': 1.263, 'grad_norm': 0.4410995841026306, 'learning_rate': 5.352694795211555e-05, 'epoch': 1.06}\n",
      "{'loss': 1.206, 'grad_norm': 0.3306519389152527, 'learning_rate': 4.911757596784357e-05, 'epoch': 1.11}\n",
      "{'loss': 1.1998, 'grad_norm': 0.27839601039886475, 'learning_rate': 4.471507142902036e-05, 'epoch': 1.16}\n",
      "{'loss': 1.2231, 'grad_norm': 0.361621230840683, 'learning_rate': 4.035369672952516e-05, 'epoch': 1.21}\n",
      "{'loss': 1.2378, 'grad_norm': 0.3388305604457855, 'learning_rate': 3.6067394171175394e-05, 'epoch': 1.26}\n",
      "{'loss': 1.2084, 'grad_norm': 0.3529285192489624, 'learning_rate': 3.188952180851589e-05, 'epoch': 1.31}\n",
      "{'loss': 1.2337, 'grad_norm': 0.36859068274497986, 'learning_rate': 2.785259384049959e-05, 'epoch': 1.36}\n",
      "{'loss': 1.2477, 'grad_norm': 0.37504133582115173, 'learning_rate': 2.3988027569455895e-05, 'epoch': 1.41}\n",
      "{'loss': 1.2922, 'grad_norm': 0.3535672128200531, 'learning_rate': 2.0325898896632177e-05, 'epoch': 1.46}\n",
      "{'loss': 1.1901, 'grad_norm': 0.3284512162208557, 'learning_rate': 1.689470825715998e-05, 'epoch': 1.51}\n",
      " 76%|███████████████████████████████          | 300/396 [43:31<13:12,  8.26s/it][INFO|trainer.py:4226] 2025-05-11 14:59:58,504 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-05-11 14:59:58,504 >>   Num examples = 454\n",
      "[INFO|trainer.py:4231] 2025-05-11 14:59:58,505 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:31,  3.50it/s]\u001b[A\n",
      "  3%|█▏                                         | 3/114 [00:01<00:44,  2.47it/s]\u001b[A\n",
      "  4%|█▌                                         | 4/114 [00:01<00:51,  2.14it/s]\u001b[A\n",
      "  4%|█▉                                         | 5/114 [00:02<00:54,  1.99it/s]\u001b[A\n",
      "  5%|██▎                                        | 6/114 [00:02<00:56,  1.90it/s]\u001b[A\n",
      "  6%|██▋                                        | 7/114 [00:03<00:57,  1.85it/s]\u001b[A\n",
      "  7%|███                                        | 8/114 [00:04<00:58,  1.82it/s]\u001b[A\n",
      "  8%|███▍                                       | 9/114 [00:04<00:58,  1.80it/s]\u001b[A\n",
      "  9%|███▋                                      | 10/114 [00:05<00:58,  1.78it/s]\u001b[A\n",
      " 10%|████                                      | 11/114 [00:05<00:58,  1.77it/s]\u001b[A\n",
      " 11%|████▍                                     | 12/114 [00:06<00:57,  1.76it/s]\u001b[A\n",
      " 11%|████▊                                     | 13/114 [00:06<00:57,  1.76it/s]\u001b[A\n",
      " 12%|█████▏                                    | 14/114 [00:07<00:56,  1.76it/s]\u001b[A\n",
      " 13%|█████▌                                    | 15/114 [00:08<00:56,  1.75it/s]\u001b[A\n",
      " 14%|█████▉                                    | 16/114 [00:08<00:55,  1.76it/s]\u001b[A\n",
      " 15%|██████▎                                   | 17/114 [00:09<00:55,  1.75it/s]\u001b[A\n",
      " 16%|██████▋                                   | 18/114 [00:09<00:54,  1.75it/s]\u001b[A\n",
      " 17%|███████                                   | 19/114 [00:10<00:54,  1.75it/s]\u001b[A\n",
      " 18%|███████▎                                  | 20/114 [00:10<00:53,  1.75it/s]\u001b[A\n",
      " 18%|███████▋                                  | 21/114 [00:11<00:53,  1.75it/s]\u001b[A\n",
      " 19%|████████                                  | 22/114 [00:11<00:52,  1.75it/s]\u001b[A\n",
      " 20%|████████▍                                 | 23/114 [00:12<00:51,  1.75it/s]\u001b[A\n",
      " 21%|████████▊                                 | 24/114 [00:13<00:51,  1.75it/s]\u001b[A\n",
      " 22%|█████████▏                                | 25/114 [00:13<00:50,  1.75it/s]\u001b[A\n",
      " 23%|█████████▌                                | 26/114 [00:14<00:50,  1.75it/s]\u001b[A\n",
      " 24%|█████████▉                                | 27/114 [00:14<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▎                               | 28/114 [00:15<00:49,  1.75it/s]\u001b[A\n",
      " 25%|██████████▋                               | 29/114 [00:15<00:48,  1.75it/s]\u001b[A\n",
      " 26%|███████████                               | 30/114 [00:16<00:47,  1.75it/s]\u001b[A\n",
      " 27%|███████████▍                              | 31/114 [00:17<00:47,  1.75it/s]\u001b[A\n",
      " 28%|███████████▊                              | 32/114 [00:17<00:46,  1.75it/s]\u001b[A\n",
      " 29%|████████████▏                             | 33/114 [00:18<00:46,  1.75it/s]\u001b[A\n",
      " 30%|████████████▌                             | 34/114 [00:18<00:45,  1.75it/s]\u001b[A\n",
      " 31%|████████████▉                             | 35/114 [00:19<00:45,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 36/114 [00:19<00:44,  1.75it/s]\u001b[A\n",
      " 32%|█████████████▋                            | 37/114 [00:20<00:43,  1.75it/s]\u001b[A\n",
      " 33%|██████████████                            | 38/114 [00:21<00:43,  1.75it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 39/114 [00:21<00:42,  1.75it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 40/114 [00:22<00:42,  1.75it/s]\u001b[A\n",
      " 36%|███████████████                           | 41/114 [00:22<00:41,  1.75it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 42/114 [00:23<00:41,  1.75it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 43/114 [00:23<00:40,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 44/114 [00:24<00:39,  1.75it/s]\u001b[A\n",
      " 39%|████████████████▌                         | 45/114 [00:25<00:39,  1.75it/s]\u001b[A\n",
      " 40%|████████████████▉                         | 46/114 [00:25<00:38,  1.75it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:26<00:38,  1.75it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:26<00:37,  1.75it/s]\u001b[A\n",
      " 43%|██████████████████                        | 49/114 [00:27<00:37,  1.75it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:27<00:36,  1.75it/s]\u001b[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:28<00:35,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:29<00:35,  1.75it/s]\u001b[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:29<00:34,  1.75it/s]\u001b[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:30<00:34,  1.75it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:30<00:33,  1.75it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:31<00:33,  1.75it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 57/114 [00:31<00:32,  1.75it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:32<00:31,  1.75it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:33<00:31,  1.75it/s]\u001b[A\n",
      " 53%|██████████████████████                    | 60/114 [00:33<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:34<00:30,  1.75it/s]\u001b[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:34<00:29,  1.75it/s]\u001b[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:35<00:29,  1.75it/s]\u001b[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:35<00:28,  1.75it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:36<00:27,  1.75it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:37<00:27,  1.75it/s]\u001b[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:37<00:26,  1.75it/s]\u001b[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:38<00:26,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:38<00:25,  1.75it/s]\u001b[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:39<00:25,  1.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:39<00:24,  1.75it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:40<00:23,  1.75it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:41<00:23,  1.75it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:41<00:22,  1.75it/s]\u001b[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:42<00:22,  1.75it/s]\u001b[A\n",
      " 67%|████████████████████████████              | 76/114 [00:42<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:43<00:21,  1.75it/s]\u001b[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:43<00:20,  1.75it/s]\u001b[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:44<00:19,  1.75it/s]\u001b[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:45<00:19,  1.75it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:45<00:18,  1.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [00:46<00:18,  1.75it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [00:46<00:17,  1.75it/s]\u001b[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [00:47<00:17,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [00:47<00:16,  1.75it/s]\u001b[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [00:48<00:16,  1.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████          | 87/114 [00:49<00:15,  1.75it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [00:49<00:14,  1.75it/s]\u001b[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [00:50<00:14,  1.75it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [00:50<00:13,  1.75it/s]\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [00:51<00:13,  1.75it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [00:51<00:12,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [00:52<00:11,  1.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [00:53<00:11,  1.75it/s]\u001b[A\n",
      " 83%|███████████████████████████████████       | 95/114 [00:53<00:10,  1.75it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [00:54<00:10,  1.75it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [00:54<00:09,  1.75it/s]\u001b[A\n",
      " 86%|████████████████████████████████████      | 98/114 [00:55<00:09,  1.75it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [00:55<00:08,  1.75it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [00:56<00:08,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [00:57<00:07,  1.75it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [00:57<00:06,  1.75it/s]\u001b[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [00:58<00:06,  1.75it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [00:58<00:05,  1.75it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [00:59<00:05,  1.75it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [00:59<00:04,  1.75it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:00<00:03,  1.75it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:01<00:03,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:01<00:02,  1.75it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:02<00:02,  1.75it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:02<00:01,  1.75it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:03<00:01,  1.75it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:03<00:00,  1.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2022424936294556, 'eval_runtime': 65.1154, 'eval_samples_per_second': 6.972, 'eval_steps_per_second': 1.751, 'epoch': 1.51}\n",
      " 76%|███████████████████████████████          | 300/396 [44:36<13:12,  8.26s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:04<00:00,  1.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3910] 2025-05-11 15:01:03,624 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/checkpoint-300\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 15:01:03,798 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 15:01:03,799 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:01:04,373 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:01:04,373 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/checkpoint-300/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:01:05,691 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:01:05,691 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "{'loss': 1.2095, 'grad_norm': 0.4308812916278839, 'learning_rate': 1.3721158816050873e-05, 'epoch': 1.56}\n",
      "{'loss': 1.1941, 'grad_norm': 0.35041117668151855, 'learning_rate': 1.0829948651407374e-05, 'epoch': 1.61}\n",
      "{'loss': 1.2211, 'grad_norm': 0.42684927582740784, 'learning_rate': 8.243578542178226e-06, 'epoch': 1.66}\n",
      "{'loss': 1.2041, 'grad_norm': 0.35427185893058777, 'learning_rate': 5.982176856345445e-06, 'epoch': 1.71}\n",
      "{'loss': 1.2268, 'grad_norm': 0.4220998287200928, 'learning_rate': 4.0633429023472e-06, 'epoch': 1.76}\n",
      "{'loss': 1.1833, 'grad_norm': 0.36330583691596985, 'learning_rate': 2.50200996285046e-06, 'epoch': 1.82}\n",
      "{'loss': 1.221, 'grad_norm': 0.40592673420906067, 'learning_rate': 1.3103290768099797e-06, 'epoch': 1.87}\n",
      "{'loss': 1.1808, 'grad_norm': 0.3986111879348755, 'learning_rate': 4.975744742772848e-07, 'epoch': 1.92}\n",
      "{'loss': 1.1676, 'grad_norm': 0.3329070508480072, 'learning_rate': 7.007139991108135e-08, 'epoch': 1.97}\n",
      "100%|█████████████████████████████████████████| 396/396 [57:51<00:00,  8.26s/it][INFO|trainer.py:3910] 2025-05-11 15:14:18,632 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/checkpoint-396\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 15:14:18,790 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 15:14:18,791 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:14:19,367 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/checkpoint-396/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:14:19,367 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/checkpoint-396/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:14:20,685 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:14:20,686 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-05-11 15:14:20,873 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3476.0819, 'train_samples_per_second': 1.828, 'train_steps_per_second': 0.114, 'train_loss': 1.2867389206934456, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████| 396/396 [57:54<00:00,  8.77s/it]\n",
      "[INFO|trainer.py:4691] 2025-05-11 15:14:20,881 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
      "[INFO|trainer.py:3910] 2025-05-11 15:14:50,587 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 15:14:50,691 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 15:14:50,692 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:14:51,491 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:14:51,492 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "[INFO|trainer.py:3910] 2025-05-11 15:14:51,682 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 15:14:51,772 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 15:14:51,773 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:14:52,579 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:14:52,580 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      1.9962\n",
      "  total_flos               = 171552826GF\n",
      "  train_loss               =      1.2867\n",
      "  train_runtime            =  0:57:56.08\n",
      "  train_samples_per_second =       1.828\n",
      "  train_steps_per_second   =       0.114\n",
      "Figure saved at: /kaggle/working/llm-finetuning/models/training_loss.png\n",
      "Figure saved at: /kaggle/working/llm-finetuning/models/training_eval_loss.png\n",
      "[WARNING|2025-05-11 15:14:55] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|trainer.py:4226] 2025-05-11 15:14:55,310 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-05-11 15:14:55,311 >>   Num examples = 454\n",
      "[INFO|trainer.py:4231] 2025-05-11 15:14:55,311 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████| 114/114 [01:04<00:00,  1.76it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9962\n",
      "  eval_loss               =     1.1986\n",
      "  eval_runtime            = 0:01:05.34\n",
      "  eval_samples_per_second =      6.947\n",
      "  eval_steps_per_second   =      1.745\n",
      "[INFO|trainer.py:3910] 2025-05-11 15:16:00,661 >> Saving model checkpoint to /kaggle/working/llm-finetuning/models/\n",
      "[INFO|configuration_utils.py:696] 2025-05-11 15:16:00,763 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-11 15:16:00,764 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-11 15:16:01,582 >> tokenizer config file saved in /kaggle/working/llm-finetuning/models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-11 15:16:01,583 >> Special tokens file saved in /kaggle/working/llm-finetuning/models/special_tokens_map.json\n",
      "[INFO|modelcard.py:449] 2025-05-11 15:16:01,845 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mlegalx-finetune-llamafactory\u001b[0m at: \u001b[34mhttps://wandb.ai/medoahmed0120-student/llamafactory/runs/62puakb8\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250511_141625-62puakb8/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd LLaMA-Factory/ && llamafactory-cli train /kaggle/working/LLaMA-Factory/examples/train_lora/summarizer_finetune.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T15:18:06.771862Z",
     "iopub.status.busy": "2025-05-11T15:18:06.771056Z",
     "iopub.status.idle": "2025-05-11T15:18:13.250960Z",
     "shell.execute_reply": "2025-05-11T15:18:13.250072Z",
     "shell.execute_reply.started": "2025-05-11T15:18:06.771831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 15:18:07.497632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746976687.517976     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746976687.523939     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype = torch_dtype\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T15:18:50.405213Z",
     "iopub.status.busy": "2025-05-11T15:18:50.404470Z",
     "iopub.status.idle": "2025-05-11T15:18:51.751512Z",
     "shell.execute_reply": "2025-05-11T15:18:51.750732Z",
     "shell.execute_reply.started": "2025-05-11T15:18:50.405186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_model_id = \"/kaggle/working/llm-finetuning/models/checkpoint-396\"\n",
    "model.load_adapter(finetuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T15:18:54.161075Z",
     "iopub.status.busy": "2025-05-11T15:18:54.160423Z",
     "iopub.status.idle": "2025-05-11T15:18:57.368734Z",
     "shell.execute_reply": "2025-05-11T15:18:57.367866Z",
     "shell.execute_reply.started": "2025-05-11T15:18:54.161053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load base model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T15:22:09.886855Z",
     "iopub.status.busy": "2025-05-11T15:22:09.886232Z",
     "iopub.status.idle": "2025-05-11T15:22:10.486031Z",
     "shell.execute_reply": "2025-05-11T15:22:10.485437Z",
     "shell.execute_reply.started": "2025-05-11T15:22:09.886832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_test(ds):\n",
    "    \n",
    "    prompts=[]\n",
    "    summary=[]\n",
    "    for rec in ds:\n",
    "        \n",
    "        combined_source = ''\n",
    "        if len(rec['sources']) > 3:\n",
    "            combined_source = '\\n'.join(rec['sources'][:3])\n",
    "        else:\n",
    "            combined_source = '\\n'.join(rec['sources'])\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\\n\".join([\n",
    "                    \"You are a professional NLP data parser.\",\n",
    "                    \"Follow the provided `Task` by the user to generate the `Output JSON`.\",\n",
    "                    \"Do not generate any introduction or conclusion.\"\n",
    "                ])\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":  \"\\n\".join([\n",
    "                    \n",
    "                     \"## Document:\",\n",
    "                    combined_source,\n",
    "        \n",
    "                    \"# Task: summarize\",\n",
    "                    \"# Output JSON:\",\n",
    "                    \"```json\"\n",
    "                ])\n",
    "            }\n",
    "        ]\n",
    "        prompts.append(prompt)\n",
    "        summary.append({\n",
    "                    'Summary_long': rec[\"summary/long\"],\n",
    "                    'Summary_short': rec[\"summary/short\"]  # Fixed key\n",
    "                })\n",
    "        \n",
    "\n",
    "    return prompts , summary\n",
    "        \n",
    "\n",
    "test_sources, summarys=prepare_data_test(ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:00:36.724386Z",
     "iopub.status.busy": "2025-05-11T16:00:36.723739Z",
     "iopub.status.idle": "2025-05-11T16:00:36.728720Z",
     "shell.execute_reply": "2025-05-11T16:00:36.728216Z",
     "shell.execute_reply.started": "2025-05-11T16:00:36.724364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_resp(messages,model,device):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "                model_inputs.input_ids,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n",
    "\n",
    "#response = generate_resp(test_sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:03:34.937301Z",
     "iopub.status.busy": "2025-05-11T16:03:34.936631Z",
     "iopub.status.idle": "2025-05-11T16:03:34.940605Z",
     "shell.execute_reply": "2025-05-11T16:03:34.939922Z",
     "shell.execute_reply.started": "2025-05-11T16:03:34.937278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_summary: {'Summary_long': \"On August 28, 2013, an indigent detainee in the Montgomery Municipal Jail filed this lawsuit in the Circuit Court of Montgomery County, Alabama. The plaintiff sued the City of Montgomery and the Honorable Milton J. Westry under 42 U.S.C. § 1983. The petitioner, represented by the Southern Poverty Law Center, requested the court quash the Municipal Court order requiring the petitioner to serve an imprisonment term of 54 days. Petitioner claimed that the Municipal Court order violated  Sixth Amendment, due process, and equal protection clause. The plaintiff was an indigent woman who accumulated $2,714.00 in fines and fees on traffic tickets that she received in 2008 and 2009. On August 20, 2013, the plaintiff was arrested and brought to the Montgomery Municipal Jail. The next day, the plaintiff appeared before Defendant Judge Westry, who told the plaintiff that she must pay $1,554.00 immediately or serve 31 days in jail. When the plaintiff informed Judge Westry that she would not be able to pay after only recently securing a part-time job, Judge Westry ordered the that her outstanding fines be converted into a term of imprisonment in the Montgomery Municipal Jail. Petitioner was not appointed a lawyer during the previously described court proceeding. \\n\\nAfter filing an emergency petition for writ of certiorari, the case was removed from state court to the United States District Court for the Middle District of Alabama on October 4, 2013. On the same day, the petitioner filed an amended complaint, seeking declaratory relief.  Another indigent detainee filed a similar suit after he was taken into custody to serve a 54 day sentence for his inability to pay his traffic fees. Cleveland v. City of Montgomery, 2014 WL 6461900, at *1 (M.D. Ala. Nov 17, 2014). On October 28, 2013,  the petitioner filed a motion to consolidate both cases. On November 12, 2013, petitioners amended their complaint and additionally sought injunctive relief.  Judge Mark E. Fuller granted the motion to consolidate both cases on November 14, 2013. \\n \\nDiscovery began on December 23, 2013. During discovery, a similar case raising comparable claims,  Mitchell v. City of Montgomery, was filed. 2014 WL 6461900, at *1 (M.D. Ala. Nov 17, 2014). The court in Mitchell v. City of Montgomery entered a preliminary injunction that ordered the City of Montgomery to submit a comprehensive set of judicial procedures to implement for the collection of future fines. On May 14, 2014, petitioners motioned to have limited participation in the Mitchell v. City of Montgomery hearing regarding the proposed judicial procedures. 2014 WL 6461900, at *2 (M.D. Ala. Nov 17, 2014). On May 27, 2014, Judge Fuller granted the petitioners’ motion in part and denied it in part. Judge Fuller determined that the petitioners were only permitted to participate in the Mitchell hearings orally and through ancillary briefings to provide perspective on the legal sufficiency of the City’s proposed plan. Cleveland v. City of Montgomery, 300 F.R.D. 578, 581 (M.D. Ala. May 27, 2014). \\n\\nFollowing the petitioners' participation, all parties decided to engage in private mediation to create judicial procedures that would satisfy federal and state-law requirements. 2014 WL 6461900, at *2 (M.D. Ala. Nov 17, 2014). On August 28, 2014, both parties submitted a joint motion to approve the proposed settlement agreement. In the initial agreement, both parties submitted an agreement to pay the plaintiffs’ attorneys fees and provide a list of basic premises and procedures that the Montgomery Municipal Court would abide by with regards to indigent defendants unable to pay any court-ordered monies, including fines, court costs, or restitution. The parties filed an amended joint motion for entry of agreed settlement order on September 12, 2014. The amended motion included a joint brief in support and requested that the court issue three declarations: <blockquote> (1) Under the current status of the law,  the constitutional principles set out in Bearden v. Georgia, 461 U.S. 660, 103 S.Ct. 2064, 76 L.Ed.2d 221 (1983), regarding incarceration for non-payment, and Turner v. Rogers, 131 S.Ct. 2507, 180 L.Ed.2d 452 (2011), regarding notice, apply in municipal-court proceedings, and that, to the extent applicable in a particular case, the judges of the Montgomery Municipal Court are legally required to follow them.\\n(2) that the proposed judicial procedures facially comply with the constitutional principles set out in Bearden, regarding incarceration for non-payment, and Turner, regarding notice. In Bearden, the Supreme Court held that, under the Fourteenth Amendment's Due Process Clause and Equal Protection Clause, a trial court cannot “automatically revok[e] probation because [a] petitioner could not pay his fine, without determining that petitioner had not made sufficient bona fide efforts to pay or that adequate alternative forms of punishment did not exist.” 461 U.S. at 662.\\n(3) the proposed judicial procedures facially comply with the requirements of the Fourteenth and Sixth Amendments to the United States Constitution, §§ 13, 64, and 225 of the Alabama Constitution, and Rule 26.11 of the Alabama Rules of Criminal Procedure. </blockquote>2014 WL 6461900, at *3-5 (M.D. Ala. Nov 17, 2014). Judge Myron H. Thompson submitted an opinion granting the parties’ joint motion for entry of agreed settlement order and subsequent final judgment on the docket on November 17, 2014. The case is closed. \\n\", 'Summary_short': \"In 2013, indigent detainee filed a complaint against the City of Montgomery and a Municipal Court Judge for unconstitutionally ordering the petitioner to serve time for her inability to pay court-order fines and fees for their traffic violations. The complaint was originally filed in the Circuit Court of Montgomery County, Alabama and then transferred to the U.S. District Court for the Middle District of Alabama, where it was consolidated with a similar case. In the amended complaint, petitioners alleged that the imprisonment orders violated their Sixth and Fourteenth Amendment Rights. Petitioners sought remedy through injunctive and declaratory relief. In 2014, the parties reached a settlement that provided new judicial procedures for the Municipal Court to follow regarding indigent defendants and nonpayment. It also included three declarations, most importantly, it declared that the constitutional principles set out in Bearden v. Georgia, 461 U.S. 660 (1983), and Turner v. Rogers, 131 S. Ct. 2507 (2011) applied to municipal court proceedings, and awarded plaintiff's attorney fees. The case closed in November 2014. \"}\n"
     ]
    }
   ],
   "source": [
    "print(f'real_summary: {summarys[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:58:09.895147Z",
     "iopub.status.busy": "2025-05-11T16:58:09.894850Z",
     "iopub.status.idle": "2025-05-11T16:59:05.608861Z",
     "shell.execute_reply": "2025-05-11T16:59:05.608285Z",
     "shell.execute_reply.started": "2025-05-11T16:58:09.895130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model_summary: {\n",
      "  \"task\": \"summarize\",\n",
      "  \"output\": {\n",
      "    \"title\": \"Summary of Harriet Delores Cleveland's Amended Complaint\",\n",
      "    \"summary\": [\n",
      "      \"Harriet Delores Cleveland appeals from her incarceration due to inability to pay traffic fines, claiming violations of the U.S. and Alabama constitutions and Alabama law.\",\n",
      "      \"She alleges the Municipal Court lacked procedural safeguards, particularly concerning her indigency and ability to pay fines.\",\n",
      "      \"Key points include: Municipal Court's denial of plea and payment plans, lack of investigation into Cleveland's indigency, failure to assign counsel, and mistreatment due to her inability to pay fines.\",\n",
      "      \"Claims include deprivation of due process, equal protection, right to counsel, and Sixth Amendment violation.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_base=generate_resp(test_sources[1],base_model, 'cuda:1')\n",
    "print(f'base_model_summary: {response_base}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:03:44.599623Z",
     "iopub.status.busy": "2025-05-11T16:03:44.599096Z",
     "iopub.status.idle": "2025-05-11T16:07:19.193929Z",
     "shell.execute_reply": "2025-05-11T16:07:19.193341Z",
     "shell.execute_reply.started": "2025-05-11T16:03:44.599599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft_model_summary: ```json\n",
      "{\"Summary_long\": \"On September 13, 2013, a 49-year-old indigent woman, represented by the Southern Poverty Law Center, filed suit in the Circuit Court of Montgomery County, Alabama, against the city of Montgomery seeking declaratory relief declaring that the jailing of the plaintiff for failure to pay her traffic ticket violated the Fourteenth Amendment\\u2019s right to due process and equal protection. Specifically, the plaintiff alleged that her lack of income to pay her traffic ticket fines meant the city had no legitimate justification to incarcerate the plaintiff.\\n\\nOn November 12, 2013, the case was transferred to the United States District Court for the Middle District of Alabama. The plaintiff filed an amended complaint on December 22, 2013, adding the plaintiff\\u2019s right to counsel as a separate cause of action. Shortly thereafter, the plaintiff moved for a temporary restraining order. On March 3, 2014, the defendants argued that a permanent injunction was inappropriate because the underlying traffic citations had been dismissed. They contended that the plaintiff\\u2019s injuries stemmed from the fact that the city had failed to inform the plaintiff that she needed to demonstrate financial hardship in order to receive an alternative sentence of community service. Additionally, the city claimed that the plaintiff lacked standing to bring the claims asserted in her complaint because her claim arose outside of the scope of the traffic citation itself. The plaintiffs responded that the city had engaged in similar behavior in the context of bail hearings, demonstrating the pattern and practice in question and asserting that the plaintiffs had suffered concrete harm.\\n\\nOn April 16, 2014, the court entered an order denying the plaintiffs\\u2019 request for preliminary injunction, finding that the plaintiffs lacked a significant risk of irreparable harm if the case were denied in favor of the defendants. The court also rejected the defendants\\u2019 argument that the plaintiffs had no standing to sue and held that the plaintiffs had standing to sue because the denial of bail services in conjunction with the issuance of a warrant for contempt of court deprived the plaintiffs of liberty and property interests protected by the Fourth Amendment. In addition, the court held that the plaintiffs had standing to sue on their right to counsel because the plaintiffs demonstrated that they had been prejudiced by the city\\u2019s alleged disregard of counsel\\u2019s role in bail reform. On May 4, 2014, the court granted the plaintiffs\\u2019 application for a temporary restraining order. The defendants appealed the decision to deny the motion for a preliminary injunction. \\n\\nOn July 21, 2014, the court granted the plaintiff\\u2019s request for a temporary restraining order and scheduled a status conference for October 26, 2014, at which point the court would decide whether to grant the plaintiff\\u2019s motion for a preliminary injunction. That day, however, the court terminated the case and referred the case to mediation. The mediation proceeded on November 3, 2014, resulting in a settlement agreement signed the next day. The settlement agreement included a promise by the city of Montgomery to implement a new bail system that would allow individuals to apply for community service sentences without having to prove financial need and to change the manner by which the court issues orders for individuals to pay monetary obligations. \", \"Summary_short\": \"Indigent women who were arrested and detained in jail for failure to pay their traffic tickets challenged their arrest and detention in federal court. The court denied the plaintiff\\u2019s request for a preliminary injunction because the underlying citations had been dismissed and denied the plaintiff\\u2019s motion for a temporary restraining order because the city failed to disclose the existence of a new bail system allowing individuals to receive community-based sentences without proving financial need. \"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response_fine=generate_resp(test_sources[1],model, 'cuda:0')\n",
    "print(f'ft_model_summary: {response_fine}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:07:53.371460Z",
     "iopub.status.busy": "2025-05-11T16:07:53.370989Z",
     "iopub.status.idle": "2025-05-11T16:07:58.327290Z",
     "shell.execute_reply": "2025-05-11T16:07:58.326590Z",
     "shell.execute_reply.started": "2025-05-11T16:07:53.371441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# First install required packages\n",
    "!pip install -qU nltk rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T16:33:34.084514Z",
     "iopub.status.busy": "2025-05-11T16:33:34.084018Z",
     "iopub.status.idle": "2025-05-11T16:33:37.725907Z",
     "shell.execute_reply": "2025-05-11T16:33:37.725176Z",
     "shell.execute_reply.started": "2025-05-11T16:33:34.084491Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.48.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:45:42.612810Z",
     "iopub.status.busy": "2025-05-11T16:45:42.612123Z",
     "iopub.status.idle": "2025-05-11T16:45:42.625997Z",
     "shell.execute_reply": "2025-05-11T16:45:42.625453Z",
     "shell.execute_reply.started": "2025-05-11T16:45:42.612787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bert_score import score as bert_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_metrics(references, predictions):\n",
    "    \"\"\"Unified text evaluation metrics\"\"\"\n",
    "    # Combine reference summaries into single text\n",
    "    ref_texts = [\n",
    "        f\"Long Summary: {ref['Summary_long']}\\nShort Summary: {ref['Summary_short']}\"\n",
    "        for ref in references\n",
    "    ]\n",
    "    \n",
    "    # Convert predictions to single text\n",
    "    pred_texts = [\n",
    "        f\"Long Summary: {pred.get('Summary_long', '')}\\nShort Summary: {pred.get('Summary_short', '')}\"\n",
    "        if isinstance(pred, dict) \n",
    "        else str(pred)\n",
    "        for pred in predictions\n",
    "    ]\n",
    "\n",
    "    # Initialize metrics dict\n",
    "    metrics = defaultdict(dict)\n",
    "\n",
    "    # BLEU with proper tokenization\n",
    "    smooth = SmoothingFunction().method4\n",
    "    metrics['bleu'] = corpus_bleu(\n",
    "        [[nltk.word_tokenize(ref)] for ref in ref_texts],\n",
    "        [nltk.word_tokenize(pred) for pred in pred_texts],\n",
    "        smoothing_function=smooth,\n",
    "        weights=(0.5, 0.3, 0.2, 0)  # Focus on lower n-grams\n",
    "    )\n",
    "    \n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = defaultdict(list)\n",
    "    \n",
    "    for ref, pred in zip(ref_texts, pred_texts):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for key in scores:\n",
    "            rouge_scores[key].append({\n",
    "                'f': scores[key].fmeasure,\n",
    "                'p': scores[key].precision,\n",
    "                'r': scores[key].recall\n",
    "            })\n",
    "    \n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score(pred_texts, ref_texts, lang='en', verbose=False)\n",
    "    \n",
    "    metrics.update({\n",
    "        'rouge': {k: {\n",
    "            'f': np.mean([v['f'] for v in vals]),\n",
    "            'p': np.mean([v['p'] for v in vals]),\n",
    "            'r': np.mean([v['r'] for v in vals])\n",
    "        } for k, vals in rouge_scores.items()},\n",
    "        'bert_score': {\n",
    "            'f1': F1.mean().item(),\n",
    "            'precision': P.mean().item(),\n",
    "            'recall': R.mean().item()\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    return dict(metrics)\n",
    "\n",
    "def parse_response(response):\n",
    "    \"\"\"Unified text extraction from model response\"\"\"\n",
    "    try:\n",
    "        # Try direct JSON parsing\n",
    "        json_str = response.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "        data = json_repair.loads(json_str)\n",
    "        \n",
    "        return (\n",
    "            f\"Long Summary: {data.get('Summary_long', '')}\\n\"\n",
    "            f\"Short Summary: {data.get('Summary_short', '')}\"\n",
    "        )\n",
    "    except:\n",
    "        # Fallback to text extraction\n",
    "        if \"```json\" in response:\n",
    "            return response.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "        return response.strip()\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_prompts, references, device, max_samples=None):\n",
    "    \"\"\"Unified text evaluation\"\"\"\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    \n",
    "    test_prompts = test_prompts[:max_samples] if max_samples else test_prompts\n",
    "    references = references[:max_samples] if max_samples else references\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for i, prompt in enumerate(tqdm(test_prompts, desc=\"Evaluating\")):\n",
    "            try:\n",
    "                with torch.device(device):\n",
    "                    response = generate_resp(prompt, model, device)\n",
    "                    parsed = parse_response(response)\n",
    "                    predictions.append(parsed)\n",
    "                    \n",
    "                if i % 5 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {str(e)}\")\n",
    "                predictions.append(\"\")\n",
    "\n",
    "    return evaluate_metrics(references, predictions)\n",
    "\n",
    "def print_metrics(name, metrics):\n",
    "    \"\"\"Simplified metric printing\"\"\"\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
    "    \n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, vals in metrics['rouge'].items():\n",
    "        print(f\"{key.upper():<8} F1={vals['f']:.4f}  P={vals['p']:.4f}  R={vals['r']:.4f}\")\n",
    "    \n",
    "    print(\"\\nBERTScore:\")\n",
    "    bert = metrics['bert_score']\n",
    "    print(f\"F1: {bert['f1']:.4f}  Precision: {bert['precision']:.4f}  Recall: {bert['recall']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T17:00:12.306358Z",
     "iopub.status.busy": "2025-05-11T17:00:12.305641Z",
     "iopub.status.idle": "2025-05-11T17:24:38.279161Z",
     "shell.execute_reply": "2025-05-11T17:24:38.278574Z",
     "shell.execute_reply.started": "2025-05-11T17:00:12.306331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db18388e3fe94ba18d1989bf2d1d4759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abc884326ab43f28a383cd186642130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sample 3: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 22.28 GiB of which 1.04 GiB is free. Process 6556 has 21.23 GiB memory in use. Of the allocated memory 17.88 GiB is allocated by PyTorch, and 3.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Metrics:\n",
      "BLEU: 0.0000\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE1   F1=0.0136  P=1.0000  R=0.0069\n",
      "ROUGE2   F1=0.0068  P=0.6667  R=0.0034\n",
      "ROUGEL   F1=0.0136  P=1.0000  R=0.0069\n",
      "\n",
      "BERTScore:\n",
      "F1: 0.7952  Precision: 0.8681  Recall: 0.7338\n",
      "\n",
      "Fine-Tuned Model Metrics:\n",
      "BLEU: 0.0979\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE1   F1=0.3827  P=0.5916  R=0.2977\n",
      "ROUGE2   F1=0.1740  P=0.2744  R=0.1341\n",
      "ROUGEL   F1=0.2061  P=0.3211  R=0.1604\n",
      "\n",
      "BERTScore:\n",
      "F1: 0.7773  Precision: 0.7849  Recall: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating Models...\")\n",
    "base_metrics = evaluate_model(base_model, base_tokenizer, test_sources, summarys, 'cuda:1', max_samples=10)\n",
    "ft_metrics = evaluate_model(model, tokenizer, test_sources, summarys, 'cuda:0', max_samples=10)\n",
    "\n",
    "print_metrics(\"Base Model\", base_metrics)\n",
    "print_metrics(\"Fine-Tuned Model\", ft_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "R4gFkGQA4_BW"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 7278629,
     "sourceId": 11604743,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7286477,
     "sourceId": 11615721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7288261,
     "sourceId": 11617984,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7375432,
     "sourceId": 11748554,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
